{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dd52654-906f-49f7-a4d3-4fd8cd1d15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from openai import OpenAI\n",
    "from bs4 import BeautifulSoup\n",
    "# from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e28ae767-27a9-48e7-84c7-0de59e785b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"]=\"127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"]=\"127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe5627-173e-448d-9f81-ab10cd253d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43695dfc-e83a-41c1-bf43-4e4a920468f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c4b31e-a98a-4cd8-b268-42d5b46ab1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "137483d2-c583-4e39-a2f5-0e7c642aaba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\"\n",
    "api_base = \"\"\n",
    "client = OpenAI(api_key=api_key, base_url=api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2958cc42-f107-4c2e-9441-ba6e3b0387e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_model = \"gpt-4o-mini\" # \"gpt-3.5-turbo\"\n",
    "embed_model = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc8829c-7585-47f4-bc04-23d3f93da84e",
   "metadata": {},
   "source": [
    "# 初始化两个Database\n",
    "预想应该有两个数据库：    \n",
    "一个数据库是完全体，包含已经计算过的嵌入，相似indices，距离了；      \n",
    "另一个是半成品，用于储存爬取下来的信息，不包含嵌入和距离之类的。注意，数据库应该在每次与完全体数据库合并之后进行清除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b04fc-f7c9-4b29-b29f-aeb132578138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化完全体向量数据库\n",
    "# ['名称', '作者', '日期', '摘要', '摘要总结', '摘要总结的嵌入']\n",
    "complete_columns = ['title', 'Author', 'Date', 'Abstract', 'Abstract_Summaried', ''Abstract_Summaried embeddings'] \n",
    "completeDatabase = pd.DataFrame(columns=complete_columns)\n",
    "completeDatabase.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669b98b-993a-42c0-90b3-545be0950fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化半成品数据库\n",
    "# ['名称', '作者', '日期', '摘要']\n",
    "semicomplete_columns = ['title', 'Author', 'Date', 'Abstract'] \n",
    "semicompleteDatabase = pd.DataFrame(columns=semicomplete_columns)\n",
    "semicompleteDatabase.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d7fb58-6339-4ca5-9adc-106e48579212",
   "metadata": {},
   "source": [
    "# 使用函数提取用户输入中的关键信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4b23981-17cf-46c2-8301-a874bb2e26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"extract_query_info\",\n",
    "            \"description\": \"Extracts query information from the user input including whether to initiate crawling, the domain that users need to query, and the specific content being queried.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"crawl\": {\n",
    "                        \"type\": \"boolean\",\n",
    "                        \"description\": \"Whether the user explicitly requests to perform crawling (e.g., by using phrases like '需要爬取').\"\n",
    "                    },\n",
    "                    \"domain\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The domain that users need to query (e.g., literature, computer science, data science, etc.)\"\n",
    "                    },\n",
    "                    \"content\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The specific content or topic within the domain being queried (e.g., 红楼梦, 位置编码, positional encoding, etc.).\"\n",
    "                    },\n",
    "                    \"major\": {\n",
    "                        \"type\": \"boolean\",\n",
    "                        \"description\": \"Is the field to be inquired related to science and engineering?\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"crawl\", \"domain\", \"content\", \"major\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"summary_abstract\",\n",
    "            \"description\": \"Summarize the abstract of the designated article and limit it to 60 words or less.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"results\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Summary of the article abstract.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"results\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6e20ba6-5e49-4168-9699-531098825700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_WVKd0H8gEojfJl6EOE09AWoZ', function=Function(arguments='{\"content\":\"位置编码\",\"crawl\":true,\"domain\":\"计算机科学\",\"major\":true}', name='extract_query_info'), type='function')])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "messages = [{\"role\": \"user\", \"content\": \"我需要爬取关于位置编码的有关内容\"}]\n",
    "response = client.chat.completions.create(model=dialogue_model, messages=messages,\n",
    "                                          tools=tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_query_info\"}})\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8e834c4-28b2-4a0a-91ce-68c52221d834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9nOjATWRlryha4haqS1UVWzsewrtA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_WVKd0H8gEojfJl6EOE09AWoZ', function=Function(arguments='{\"content\":\"位置编码\",\"crawl\":true,\"domain\":\"计算机科学\",\"major\":true}', name='extract_query_info'), type='function')]))], created=1721560152, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_8b761cb050', usage=CompletionUsage(completion_tokens=20, prompt_tokens=225, total_tokens=245))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605de177-c796-4ac3-9987-00ed60e98c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "print(\n",
    "    response.choices[0].message.tool_calls, \"\\n\", \\\n",
    "    response.choices[0].message.tool_calls[0].function.arguments, \"\\n\", \\\n",
    "    json.loads(response.choices[0].message.tool_calls[0].function.arguments)[\"domain\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d52230-ca40-44d5-ab4f-b2492a811818",
   "metadata": {},
   "source": [
    "# 向量化用户请求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a71e10-8277-432f-9484-b049846822db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(request):\n",
    "    \"\"\"\n",
    "    request是用户的输入，是一个string\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(input=request, model=embed_model)\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b43a9af-23fa-4634-bdcd-2dfe8d944d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "request_embeddings = get_query_embedding(\"cat is not dag\")\n",
    "len(request_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b048dc20-ea25-4cc2-a282-f258b53bff59",
   "metadata": {},
   "source": [
    "# 爬虫\n",
    "已经有了json.loads(response.choices[0].message.tool_calls[0].function.arguments)，类似：{\"content\":\"位置编码\",\"crawl\":true,\"domain\":\"科学与工程\",\"major\":true}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92d26a-fe80-44ae-acbc-65dc5027d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "print(json.loads(response.choices[0].message.tool_calls[0].function.arguments), \"\\n\", type(json.loads(response.choices[0].message.tool_calls[0].function.arguments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76a6b58-bd49-4afb-a7b2-32a2b7739ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_urls = {\n",
    "    \"ACM\": \"\",\n",
    "    \"知网空间\": 'http://search.cnki.com.cn/Search/ListResult'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6115b-445c-4d46-927c-a9f7f33b975f",
   "metadata": {},
   "source": [
    "### 知网 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b33a7077-17fe-4081-a4ed-91bc81bd7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'}\n",
    "# Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 Edg/128.0.0.0\n",
    "def get_page_text(url, search_word, page_num):\n",
    "    data = {\n",
    "        'searchType': 'MulityTermsSearch', 'ArticleType': '', 'ReSearch': '', 'ParamIsNullOrEmpty': 'false', 'Islegal': 'false',\n",
    "        'Content': search_word,\n",
    "        'Theme': '', \n",
    "        'Title': '', 'KeyWd': '', 'Author': '', 'SearchFund': '', 'Originate': '', 'Summary': '', 'PublishTimeBegin': '', 'PublishTimeEnd': '', \n",
    "        'MapNumber': '', 'Name': '', 'Issn': '', 'Cn': '', 'Unit': '', 'Public': '', 'Boss': '', 'FirstBoss': '', 'Catalog': '', 'Reference': '', 'Speciality': '', \n",
    "        'Type': '', 'Subject': '', 'SpecialityCode': '', 'UnitCode': '', 'Year': '', 'AcefuthorFilter': '', 'BossCode': '', 'Fund': '', 'Level': '', 'Elite': '', \n",
    "        'Organization': '', 'Order': '1',\n",
    "        'Page': str(page_num),\n",
    "        'PageIndex': '', 'ExcludeField': '', 'ZtCode': '', 'Smarts': '',\n",
    "    }\n",
    "    response = requests.post(url=url, headers=headers, data=data)\n",
    "    page_text = response.text\n",
    "    return page_text\n",
    "\n",
    "def list_to_str(my_list):\n",
    "    my_str = \"\".join(my_list)\n",
    "    return my_str\n",
    "\n",
    "def get_abstract(url):\n",
    "    response = requests.get(url=url, headers=headers)\n",
    "    page_text = response.text\n",
    "    tree = etree.HTML(page_text)\n",
    "    abstract = tree.xpath('//div[@class=\"xx_font\"]//text()')\n",
    "    return abstract\n",
    "\n",
    "def parse_page_text(page_text, top_n):\n",
    "    tree = etree.HTML(page_text)\n",
    "    item_list = tree.xpath('//div[@class=\"list-item\"]')\n",
    "    # print(type(item_list), len(item_list)) # list类型，长度为20\n",
    "    page_info = []\n",
    "    for item in item_list[0:top_n]: # 只解析前top_n篇\n",
    "        # 标题\n",
    "        title = list_to_str(item.xpath(\n",
    "            './p[@class=\"tit clearfix\"]/a[@class=\"left\"]/@title'))\n",
    "        # 链接\n",
    "        link = 'https:' +\\\n",
    "            list_to_str(item.xpath(\n",
    "                './p[@class=\"tit clearfix\"]/a[@class=\"left\"]/@href'))\n",
    "        # 作者\n",
    "        author = list_to_str(item.xpath(\n",
    "            './p[@class=\"source\"]/span[1]/@title'))\n",
    "        # 出版日期\n",
    "        date = list_to_str(item.xpath(\n",
    "            './p[@class=\"source\"]/span[last()-1]/text() | ./p[@class=\"source\"]/a[2]/span[1]/text() '))\n",
    "        # 关键词\n",
    "        keywords = list_to_str(item.xpath(\n",
    "            './div[@class=\"info\"]/p[@class=\"info_left left\"]/a[1]/@data-key'))\n",
    "        # 摘要\n",
    "        abstract = list_to_str(get_abstract(url=link))\n",
    "        # 文献来源\n",
    "        paper_source = list_to_str(item.xpath(\n",
    "            './p[@class=\"source\"]/span[last()-2]/text() | ./p[@class=\"source\"]/a[1]/span[1]/text() '))\n",
    "        # 文献类型\n",
    "        paper_type = list_to_str(item.xpath(\n",
    "            './p[@class=\"source\"]/span[last()]/text()'))\n",
    "        # 下载量\n",
    "        download = list_to_str(item.xpath(\n",
    "            './div[@class=\"info\"]/p[@class=\"info_right right\"]/span[@class=\"time1\"]/text()'))\n",
    "        # 被引量\n",
    "        refer = list_to_str(item.xpath(\n",
    "            './div[@class=\"info\"]/p[@class=\"info_right right\"]/span[@class=\"time2\"]/text()'))\n",
    "\n",
    "        item_info = [i.strip() for i in [title, author, paper_source, paper_type, date, abstract, keywords, download, refer, link]]\n",
    "        page_info.append(item_info)\n",
    "        # print(page_info)\n",
    "    return page_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8271925b-03ae-472b-99e8-d6a3b8b02647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "page_text = get_page_text(base_urls[\"知网空间\"], \"位置编码\", 1)\n",
    "page_info = parse_page_text(page_text, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff342a8b-df55-4682-9fc3-d98dc58aef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "print(len(page_info), page_info[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ff2da-d8db-4157-bef4-83b097dad7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "for paper_info in page_info:\n",
    "    paper_info_selected = [paper_info[i] for i in [0, 1, 4, 5]] # title, author, date, abstract\n",
    "    new_row = pd.DataFrame([dict(zip(semicomplete_columns, paper_info_selected))])\n",
    "    semicompleteDatabase = pd.concat([semicompleteDatabase, new_row], ignore_index=True)\n",
    "    \n",
    "semicompleteDatabase.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426a5c2-2f39-41a1-af59-1575fb0e1106",
   "metadata": {},
   "outputs": [],
   "source": [
    "for abstract in semicompleteDatabase[\"Abstract\"]:\n",
    "    print(abstract)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a56a51-cced-4b96-a769-356ae6e58c79",
   "metadata": {},
   "source": [
    "# 将摘要传给模型，让模型总结一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939ec88-275d-47d6-9f9a-ed667633f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "for abstract in semicompleteDatabase[\"Abstract\"]:\n",
    "    messages = [{\"role\": \"system\", \"content\": \"你是一位擅长文字总结的助手。我将发送给你一篇文章的摘要，你需要总结它，并限制在60个字以内。\"},\n",
    "                {\"role\": \"user\", \"content\": \"你需要总结的摘要如下：\"+abstract}]\n",
    "    response = client.chat.completions.create(model=dialogue_model, messages=messages, \n",
    "                                              tools=tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"summary_abstract\"}})\n",
    "    print(response.choices[0].message)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1dbb2e-e7c4-4f40-89b1-07b2bf142cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.tool_calls[0].function.arguments, \"\\n\", json.loads(response.choices[0].message.tool_calls[0].function.arguments)['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a87282-80b5-4b1b-9ded-4eca9e8773f2",
   "metadata": {},
   "source": [
    "# 处理domain和content中存在空格的现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d4be6c-5ddd-4780-9403-5661922d6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_spaces_with_plus(input_string):\n",
    "    if ' ' in input_string:\n",
    "        return input_string.replace(' ', '+')\n",
    "    else:\n",
    "        return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a65d1-5a02-4cbb-ac17-b6190e407cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebe8a7cf-8edb-49d0-823d-f2d1375719a8",
   "metadata": {},
   "source": [
    "### ACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1f349-4010-4bcf-b004-ffd5868de864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crawler_ACM(domain, content, num_pages):\n",
    "    entries = []\n",
    "    i=0\n",
    "    while i < num_pages: # 爬取几页\n",
    "        url = \"https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&field1=AllField&text1=\"+domain+\"&field2=AllField&text2=\"+content+\"&startPage=\"+str(i)+\"&pageSize=20\"\n",
    "        print(url)\n",
    "        content = requests.get(url).text\n",
    "        page = BeautifulSoup(content, 'lxml')\n",
    "        for entry in page.find_all(\"div\", attrs={\"class\": \"issue-item__content\"}):\n",
    "            entries.append(entry)\n",
    "\n",
    "        i+=1\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb6f6a-b144-4001-810d-f4da62c272a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"positional+encoding\"\n",
    "content = \"CNN\"\n",
    "num_pages = 1\n",
    "\n",
    "results = Crawler_ACM(domain, content, num_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cf3e90-01e7-4242-8eab-8b93e517203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6591df5-3dd2-4a79-846f-a6df4ae21980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crawler_ACM(domain, content, num_pages):\n",
    "    results = []\n",
    "    i=0\n",
    "    while i < num_pages: # 爬取几页\n",
    "        url = \"https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&field1=AllField&text1=\"+domain+\"&field2=AllField&text2=\"+content+\"&startPage=\"+str(i)+\"&pageSize=20\"\n",
    "        print(url)\n",
    "        content = requests.get(url).text\n",
    "        page = BeautifulSoup(content, 'lxml')\n",
    "        for entry in page.find_all(\"div\", attrs={\"class\": \"issue-item__content\"}):\n",
    "            title = entry.find('h5', attrs={'class': 'issue-item__title'})\n",
    "            author = entry.find('ul', attrs={'class': 'rlist--inline'})\n",
    "            years = entry.find('span', attrs={'class': 'dot-separator'})\n",
    "            \n",
    "            print(type(title), type(author), type(years), title, author, years)\n",
    "            break\n",
    "\n",
    "            # 暂时存储中间结果\n",
    "            results.append({\n",
    "                \"title\": title.text.replace('[PDF]', '').strip() if title else '',\n",
    "                'authors': author.text.replace('\\n', '').strip() if author else '',\n",
    "                'years': years.text.split(',')[0].split(' ')[1].strip() if years else '',\n",
    "                'abstract': ''  # 先初始化为空字符串\n",
    "            })\n",
    "            \n",
    "        abstract_entries = page.find_all(\"div\", attrs={\"class\": \"accordion__content\"})\n",
    "        for i, abst_entry in enumerate(abstract_entries):\n",
    "            abst = abst_entry.find('div', attrs={'class': 'abstract-text'})\n",
    "            if i < len(results):\n",
    "                results[i]['abstract'] = abst.text.replace('\\n', '').strip() if abst else ''\n",
    "                \n",
    "        # Add a sleep of 1 seconds between requests\n",
    "        time.sleep(3)\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52db4f-2802-4da8-966c-298ea2f00578",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"positional+encoding\"\n",
    "content = \"CNN\"\n",
    "num_pages = 1\n",
    "\n",
    "results = Crawler_ACM(domain, content, num_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad437fc2-140f-4217-b475-a5e8960a84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e3a6a-531e-4936-a698-9a9cd195ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a194f6-0536-4003-81e7-41cd091a21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper_info in results:\n",
    "    paper_info_selected = [paper_info[i] for i in [\"title\", 'authors', 'years', 'abstract']] # title, author, date, abstract\n",
    "    new_row = pd.DataFrame([dict(zip(semicomplete_columns, paper_info_selected))])\n",
    "    semicompleteDatabase = pd.concat([semicompleteDatabase, new_row], ignore_index=True)\n",
    "    \n",
    "semicompleteDatabase.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2456613b-2b9b-4fce-87dc-705c3c8492f8",
   "metadata": {},
   "source": [
    "## 汇总爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f9567-b854-4617-b33f-78f789d94aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(request_vector, params):\n",
    "    \"\"\"\n",
    "    url：用户需要爬取网站的url，例如：\n",
    "    params：第一次将用户请求输入模型的输出中的字典。与url结合。例如：https://httpbin.org/get?key1=value1&key2=value2\n",
    "    \"\"\"\n",
    "    crawl = params['crawl'] # 用户是否指明需要爬取\n",
    "    domain = params['domain'] # 领域\n",
    "    content = params['content'] # 领域内具体内容\n",
    "    major = params['major'] # 是否为理工科\n",
    "\n",
    "    urlparams = {'domain': domain, 'content': content}\n",
    "\n",
    "    if crawl: # 如果指定需要爬取\n",
    "        if major: # 如果是理工科，则使用ACM查询\n",
    "            pass\n",
    "        else: # 文科用知网查询\n",
    "            page_info = parse_page_text(get_page_text(base_urls[\"知网空间\"], \"domain + content\", 1), 5) # 只爬取前5篇\n",
    "            for paper_info in page_info:\n",
    "                paper_info_selected = [paper_info[i] for i in [0, 1, 4, 5]] # title, author, date, abstract\n",
    "                new_row = pd.DataFrame([dict(zip(semicomplete_columns, paper_info_selected))])\n",
    "                semicompleteDatabase = pd.concat([semicompleteDatabase, new_row], ignore_index=True)\n",
    "    else: # 如果没有指定，则调用find_similarParagraphs()计算用户请求与completeDatabase中Abstract列的相似度\n",
    "        similarParagraphs = find_similarParagraphs(request_vector)\n",
    "        if len(similarParagraphs) == 0:\n",
    "            if major: # 如果是理工科，则使用ACM查询\n",
    "                pass\n",
    "            else: # 文科用知网查询\n",
    "                page_info = parse_page_text(get_page_text(base_urls[\"知网空间\"], \"domain + content\", 1), 5) # 只爬取前5篇\n",
    "                for paper_info in page_info:\n",
    "                    paper_info_selected = [paper_info[i] for i in [0, 1, 4, 5]] # title, author, date, abstract\n",
    "                    new_row = pd.DataFrame([dict(zip(semicomplete_columns, paper_info_selected))])\n",
    "                    semicompleteDatabase = pd.concat([semicompleteDatabase, new_row], ignore_index=True)\n",
    "    \n",
    "    return semicompleteDatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6e7104-c21f-401e-aa27-120057545795",
   "metadata": {},
   "source": [
    "# 将semicompleteDatabase中的Abstract列使用嵌入模型进行嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f7def-e3a6-4742-a38d-e2195ec88c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text):\n",
    "    # Create embeddings for each document chunk\n",
    "    embeddings = openai.embeddings.create(input = text, model=embed_model).data[0].embedding\n",
    "    return embeddings\n",
    "\n",
    "embeddings = []\n",
    "for Abstract in flattened_df['Abstract']:\n",
    "    embeddings.append(create_embeddings(Abstract))\n",
    "\n",
    "# store the embeddings in the dataframe\n",
    "semicompleteDatabase['Abstract embeddings'] = embeddings\n",
    "\n",
    "# 整合semicompleteDatabase和completeDatabase\n",
    "completeDatabase = pd.concat([completeDatabase, semicompleteDatabase])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7221dd-7d9f-43d0-a412-7cfe3087417d",
   "metadata": {},
   "source": [
    "# 计算request和completeDatabase中Abstract列中每个元素的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486c3bd-c84c-48dc-ac4c-18ecc6fd503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarParagraphs(request_vector, threshold=0.6, top_n=5):\n",
    "    paragraph_embeddings = completeDatabase['Paragraph embeddings'].to_list()\n",
    "    nbrs = NearestNeighbors(n_neighbors=top_n, algorithm='ball_tree').fit(paragraph_embeddings) # 这里每次调用这个函数都会计算一次，maybe会增加时间成本\n",
    "    \n",
    "    distances, indices = nbrs.kneighbors([request_vector])\n",
    "    \n",
    "    similarParagraphs = []\n",
    "    for index, distance in zip(indices[0], distances[0]):\n",
    "        if distance < threshold:\n",
    "            similarParagraphs.append(completeDatabase['Abstract'].iloc[index])\n",
    "\n",
    "    if len(similarParagraphs) == 0:\n",
    "        return \"需要使用爬虫爬其，请等待！\" #############################################################################################################\n",
    "    else: \n",
    "        return similarParagraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba9e8b-a773-4dbc-a631-7a69a36d8836",
   "metadata": {},
   "source": [
    "# 将返回的similarParagraphs（list）作为prompt一起传入模型中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a071be-3b46-4531-ab32-f9f3d1cd8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(request):\n",
    "\n",
    "    request_vector = get_query_embedding(request)\n",
    "\n",
    "    similarParagraphs = find_similarParagraphs(request_vector)\n",
    "    \n",
    "    total_content = \n",
    "    \n",
    "    # create a message object\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assiatant that helps with some questions.\"},\n",
    "        {\"role\": \"user\", \"content\": total_content}\n",
    "    ]\n",
    "\n",
    "    # use chat completion to generate a response\n",
    "    response = openai.chat.completions.create(\n",
    "        model=dialogue_model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=800,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31275eb-d560-4522-9933-1cc5490afc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c92559-96a2-4222-a2d2-53acf740d903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e9b5e-5db7-44c5-b295-190c57cbeb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0195b-dc00-4b47-82ce-cf54a6d5b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afabf62d-f98c-4378-b8a7-5b781290ce17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e680aa-263b-453d-88cd-e0721cf8e910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c4139-5ba1-4b33-8ba1-41648cf62235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
