{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca977fb-eed4-45ea-8071-a93d7f1418e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"]=\"127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"]=\"127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7059b-b09c-437f-9672-9c9379b24528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 官方API\n",
    "# client = OpenAI()\n",
    "\n",
    "# 第三方低价API\n",
    "api_key = \"\"\n",
    "api_base = \"\"\n",
    "client = OpenAI(api_key=api_key, base_url=api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d739df-d27d-4784-975b-44c2bec91b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telebot\n",
    "\n",
    "BOT_TOKEN = \"\"\n",
    "bot = telebot.TeleBot(BOT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b5102-34f6-431a-b40c-d5b969742210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"telebot\"\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430c6a4-1a36-4f65-bfdf-e9706524628f",
   "metadata": {},
   "source": [
    "# /crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946c9ea-fb61-4bbd-b1dc-4ae7bd473843",
   "metadata": {},
   "outputs": [],
   "source": [
    "@bot.message_handler(commands=['crawl'])\n",
    "def initiate_crawl(message):\n",
    "    text = \"您好！在爬取之前您需要回答以下问题：\\n1.您需要爬取的文章的内容与什么有关？\\n2.您需要爬取多少篇文章？\\n3.您是否需要限制爬取的文章为几几年之后发表的？如果需要限制，请显式指出年份信息。\\n4.您需要在中国知网还是谷歌学术查询论文？\\n注意：\\n1.请使用英文进行说明。\\n2.如果不限制年份则默认从2000年开始。如果不限制查询数量，则默认10。\\n3.建议查询不超过20篇文章，因为爬取非常耗费时间。\"\n",
    "    sent_msg = bot.send_message(message.chat.id, text, parse_mode=\"Markdown\")\n",
    "    bot.register_next_step_handler(sent_msg, process_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91967c-af8a-47f4-a6e6-4f72a07eb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "response_format_extract_keywords_and_fields_and_as_ylo_and_number_and_source = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"response_format_extract_keywords_fields_as_ylo_number_source\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"keywords_for_searching\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"从用户请求中提取用于查询文章的关键字。例子: User request: 我需要爬取26篇有关全球胜任力的文章，并且最好都是2020年之后发表的。最好从谷歌学术上爬取。 - Extracted keywords: 全球胜任力\"\n",
    "                },\n",
    "                \"fields_for_searching\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"从用户请求中判断用户需要查询的是哪一个领域的文章。例子: User request: 我需要爬取26篇有关全球胜任力的文章，并且最好都是2020年之后发表的。最好从谷歌学术上爬取。 - Extracted fields: Eduction\",\n",
    "                    \"enum\": [\"Education\", \"Computer_Science\", \"Medicine\", \"Literature\", \"Other\"]\n",
    "                },\n",
    "                \"as_ylo_for_searching\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"从用户请求中提取的年份信息，提取结果必须为数字。例子: User request: 我需要爬取26篇有关全球胜任力的文章，并且最好都是2020年之后发表的。最好从谷歌学术上爬取。 - Extracted as_ylo: 2020\"\n",
    "                },\n",
    "                \"number_for_searching\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"从用户请求中提取用户需要爬取多少篇文章，提取结果必须为数字。例子: User request: 我需要爬取26篇有关全球胜任力的文章，并且最好都是2020年之后发表的。最好从谷歌学术上爬取。 - Extracted number: 26\"\n",
    "                },\n",
    "                \"source_for_searching\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"从用户请求中提取用户想要查询论文的来源。例子: User request: 我需要爬取26篇有关全球胜任力的文章，并且最好都是2020年之后发表的。最好从谷歌学术上爬取。 - Extracted source: Google_Scholar\",\n",
    "                    \"enum\": [\"CNKI\", \"Google_Scholar\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"keywords_for_searching\", \"fields_for_searching\", \"as_ylo_for_searching\", \"number_for_searching\", \"source_for_searching\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "class Keywords_and_Fields_and_As_Ylo_and_Number_and_Source(BaseModel):\n",
    "    keywords_for_searching: str\n",
    "    fields_for_searching: str\n",
    "    as_ylo_for_searching: str\n",
    "    number_for_searching: str\n",
    "    source_for_searching: str\n",
    "\n",
    "response_format_final_fields = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"response_format_final_fields\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"final_fields\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"参考我们训练的文本分类模型的预测和gpt-4o-mini-2024-07-18的预测，并根据原始的user_request，对用户需要查询的是哪一个领域的文章进行最终预测。\",\n",
    "                    \"enum\": [\"Education\", \"Computer_Science\", \"Medicine\", \"Literature\", \"Other\"]\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"final_fields\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "class Final_Fields(BaseModel):\n",
    "    final_fields: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98bab0-442e-4f9b-9ff3-faa990913968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from requests.exceptions import SSLError\n",
    "\n",
    "from request_processor import request_processor\n",
    "\n",
    "def process_query(message): \n",
    "    user_request = message.text # 用户请求类似 \"I need to crawl 26 articles about global competency, preferably all published after 2020. Better to crawl it from Google Scholar.\"\n",
    "\n",
    "    messages_to_model=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert at structured data extraction. You will be given unstructured text from a user request and should convert it into the given structure.\"},\n",
    "        {\"role\": \"user\", \"content\": user_request}\n",
    "    ]\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-mini-2024-07-18\", messages=messages_to_model, response_format=response_format_extract_keywords_and_fields_and_as_ylo_and_number_and_source)\n",
    "    \n",
    "    try:\n",
    "        keywords_and_fields_and_as_ylo_and_number_and_source = Keywords_and_Fields_and_As_Ylo_and_Number_and_Source.parse_raw(response.choices[0].message.content)\n",
    "        # print(\"@@@@@@\", keywords_and_fields_and_as_ylo_and_number)\n",
    "        keywords_and_fields_and_as_ylo_and_number_and_source_dict = keywords_and_fields_and_as_ylo_and_number_and_source.dict()\n",
    "        keywords = keywords_and_fields_and_as_ylo_and_number_and_source_dict[\"keywords_for_searching\"] # str\n",
    "        fields = keywords_and_fields_and_as_ylo_and_number_and_source_dict[\"fields_for_searching\"] # str\n",
    "        as_ylo = keywords_and_fields_and_as_ylo_and_number_and_source_dict[\"as_ylo_for_searching\"] # str\n",
    "        number = keywords_and_fields_and_as_ylo_and_number_and_source_dict[\"number_for_searching\"] # str\n",
    "        source = keywords_and_fields_and_as_ylo_and_number_and_source_dict[\"source_for_searching\"] # str\n",
    "        # print(\"******\", keywords, fields, as_ylo, number)\n",
    "    except ValidationError as e:\n",
    "        print(\"### Keywords_and_Fields_and_As_Ylo_and_Number.parse_raw(response.choices[0].message.content) 出现错误 ###\", e.json())\n",
    "\n",
    "    ########################## gpt-4o-mini + 文本分类模型 -> gpt-4o 综合判断 ##########################\n",
    "    predicted_field, predicted_prob = request_processor(user_request) # 使用我们的文本分类模型判断用户请求属于哪一个fields\n",
    "    predicted_field, predicted_prob = str(predicted_field), str(predicted_prob)\n",
    "    \n",
    "    user_messages_for_final_fields = f\"用户请求为：{user_request}。我们训练的文本分类模型判断用户请求属于：{predicted_field}。gpt-4o-mini判断用户请求属于：{fields}\"\n",
    "    \n",
    "    # 将gpt-4o-mini-2024-07-18判断的fields和文本分类模型判断的fields发送给gpt-4o-2024-08-06(使用此版本仅为了为了适应json格式的输出)，让gpt-4o-2024-08-06最终判断一下fields\n",
    "    messages_for_final_fields = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in text classification and decision-making. You will be provided with a user request, along with predictions from two models regarding the likely field or category of the request. Your task is to analyze the provided information and make a final, authoritative decision on which category the user request belongs to. Consider the input from both models carefully before making your decision.\"},\n",
    "        {\"role\": \"user\", \"content\": user_messages_for_final_fields}\n",
    "    ]\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-2024-08-06\", messages=messages_for_final_fields, response_format=response_format_final_fields)\n",
    "\n",
    "    try:\n",
    "        final_fields = Final_Fields.parse_raw(response.choices[0].message.content)\n",
    "        final_fields_dict = final_fields.dict()\n",
    "        final_fields = final_fields_dict[\"final_fields\"]\n",
    "    except ValidationError as e:\n",
    "        print(\"### Final_Fields.parse_raw(response.choices[0].message.content) 出现错误 ###\", e.json())\n",
    "    ########################## gpt-4o-mini + 文本分类模型 -> gpt-4o 综合判断 ##########################\n",
    "    \n",
    "    # 如果是无所谓数量和年份，则返回为空，我们必须处理这种情况\n",
    "    if not as_ylo:  # 检查 as_ylo 是否为 None 或者空字符串\n",
    "        # print(\"as_ylo is None or empty, 默认使用2000\")\n",
    "        as_ylo = \"2000\"\n",
    "    if not number:  # 检查 number 是否为 None 或者空字符串\n",
    "        # print(\"number is None or empty, 默认使用10\")\n",
    "        number = \"10\"\n",
    "\n",
    "    text = f\"我们判断您需要从{source}查询{final_fields}领域的，与{keywords}有关的从{as_ylo}开始的{number}篇文章。请问是否正确，如果正确我们将开始查询，如果错误，请即使指出！\"\n",
    "    # 为防止网络问题导致的问题，使用try提高下容错\n",
    "    try:\n",
    "        sent_msg = bot.send_message(message.chat.id, text, parse_mode=None)\n",
    "    except SSLError as e:\n",
    "        print(f\"SSL error occurred when bot.send_message(): {e}. Retrying...\")\n",
    "        time.sleep(5)  # 等待几秒后重试\n",
    "        sent_msg = bot.send_message(message.chat.id, text, parse_mode=None)\n",
    "\n",
    "    bot.register_next_step_handler(sent_msg, confirm_and_show_extracted_information, keywords, final_fields, as_ylo, number, source, user_request, fields, predicted_field) # gpt4omini - fields; 文本分类模型 - predicted_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6a33c-aed0-4480-82e5-3c434ce58fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据用户反馈判断提取出来的信息是否出错\n",
    "response_format_is_correct = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"response_format_is_correct\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"is_correct\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Indicates whether the user approves the extracted information.\",\n",
    "                    \"enum\": [True, False]\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"is_correct\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "class UserApproval(BaseModel):\n",
    "    is_correct: bool\n",
    "\n",
    "# 加入提取出来的信息出现了错误，则根据提取出来的信息和用户反馈对错误信息进行修正\n",
    "response_format_true_info = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"response_format_true_real_info\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"true_keywords\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The confirmed or corrected keywords based on the extracted information and the user's feedback.\"\n",
    "                },\n",
    "                \"true_fields\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The confirmed or corrected field based on the extracted information and the user's feedback.\",\n",
    "                    \"enum\": [\"Education\", \"Computer_Science\", \"Medicine\", \"Literature\", \"Other\"]\n",
    "                },\n",
    "                \"true_as_ylo\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The confirmed or corrected as_ylo (year lower bound) based on the extracted information and the user's feedback.\"\n",
    "                },\n",
    "                \"true_number\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The confirmed or corrected number of articles to crawl based on the extracted information and the user's feedback.\"\n",
    "                },\n",
    "                \"true_source\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The confirmed or corrected source (e.g., CNKI, Google Scholar) based on the extracted information and the user's feedback.\",\n",
    "                    \"enum\": [\"CNKI\", \"Google_Scholar\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"true_keywords\", \"true_fields\", \"true_as_ylo\", \"true_number\", \"true_source\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "class TrueInfo(BaseModel):\n",
    "    true_keywords: str\n",
    "    true_fields: str\n",
    "    true_as_ylo: str\n",
    "    true_number: str\n",
    "    true_source: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ceb97-f91b-4c7e-89bf-a419a7f8b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AppendRowtoCSV import append_row_to_csv\n",
    "\n",
    "def confirm_and_show_extracted_information(message, keywords, final_fields, as_ylo, number, source, user_request, fields, predicted_field): # gpt4omini - fields; 文本分类模型 - predicted_field\n",
    "    message_text = message.text  # 示例：1.判断正确的。2.不对，是有关计算机领域！\n",
    "\n",
    "    messages_to_model=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in information validation. The user has been presented with extracted information (keywords, fields, etc.). Your task is to analyze the user's feedback and determine whether they confirm the correctness of the presented information or not.\"},\n",
    "        {\"role\": \"user\", \"content\": message_text}\n",
    "    ]\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-2024-08-06\", messages=messages_to_model, response_format=response_format_is_correct)\n",
    "\n",
    "    try:\n",
    "        is_correct = UserApproval.parse_raw(response.choices[0].message.content)\n",
    "        is_correct_dict = is_correct.dict()\n",
    "        is_correct = is_correct_dict[\"is_correct\"]\n",
    "    except ValidationError as e:\n",
    "        print(\"### UserApproval.parse_raw(response.choices[0].message.content) 出现错误 ###\", e.json())\n",
    "        return\n",
    "\n",
    "    # 如果正确 ###############################################################################################################################\n",
    "    if is_correct: \n",
    "        \n",
    "        if predicted_field != final_fields: # 如果用户认为final_fields正确，但我们的文本分类模型判断错了，则需要记录一下\n",
    "            csv_file = f'data/data_misclassified_user_requests.csv'\n",
    "            new_row = {'Text': user_request, 'Field_true': final_fields, \"Field_predictedbyModel\": predicted_field}\n",
    "            append_row_to_csv(csv_file, new_row)\n",
    "        if fields != final_fields: # 如果用户认为final_fields正确，但gpt-4o-mini判断错了，则也需要记录一下 - 暂时没写 - 暂时规划：判断错的信息作为prompt返回给gpt-4o-mini\n",
    "            pass\n",
    "            \n",
    "        bot.send_message(message.chat.id, \"Very Good! 开始查询，请稍后。\", parse_mode=None)\n",
    "        \n",
    "        #################################### 爬取 ####################################\n",
    "        df_crawled = crawl_details(source, keywords, final_fields, as_ylo, number)\n",
    "        #################################### 爬取 ####################################\n",
    "        \n",
    "        bot.send_message(message.chat.id, \"查询完成，请等待模型总结查询到的内容。\", parse_mode=None)\n",
    "        ###### 展示搜索到的所有结果 ######\n",
    "        crawled_text = convert_csv_to_text(df_crawled, source)\n",
    "        # print(\"!!!\", crawled_text)\n",
    "        # 将查询的到结果进行简化\n",
    "        if crawled_text == \"网站暂时无法爬取，请稍后！\": # 与 convert_csv_to_text() 中对应\n",
    "            bot.send_message(message.chat.id, \"网站暂时无法爬取，请稍后！\", parse_mode=None)\n",
    "        else:\n",
    "            summarized_text = summarize_and_format_articles(crawled_text, source)\n",
    "            # print(\"@@@\", summarized_text)\n",
    "            # 将summarized_text拆分成最大长度为4096的chunk进行发送\n",
    "            text_chunks = split_text_into_chunks(summarized_text)\n",
    "            for chunk in text_chunks:\n",
    "                bot.send_message(message.chat.id, chunk, parse_mode=\"Markdown\")\n",
    "            sent_msg = bot.send_message(message.chat.id, \"以上是本次全部的查询内容。本次查询的数据储存在临时数据库中，是否需要合并到永久数据库中？\\n合并可用于类似情况的推荐。\", parse_mode=\"Markdown\")\n",
    "            bot.register_next_step_handler(sent_msg, merge_data, final_fields)\n",
    "    \n",
    "    # 如果错误 ###############################################################################################################################\n",
    "    else: \n",
    "        text = f\"我们提取出来的信息为：1.用于查询的关键字：{keywords}；2.查询属于的领域：{final_fields}；3.文章的发表年份需要在{as_ylo}年以后；4.需要爬取{number}篇文章。5.需要从{source}查询文章。用户对我们提取的信息的反馈为：{message_text}\"\n",
    "        messages_to_model = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in information validation. Based on the user's feedback, correct only the parts of the extracted information that are incorrect and confirm the others.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "        response = client.chat.completions.create(model=\"gpt-4o-2024-08-06\", messages=messages_to_model, response_format=response_format_true_info)\n",
    "\n",
    "        try:\n",
    "            true_info = TrueInfo.parse_raw(response.choices[0].message.content)\n",
    "            true_info_dict = true_info.dict()\n",
    "            keywords = true_info_dict[\"true_keywords\"] # str\n",
    "            final_fields = true_info_dict[\"true_fields\"] # str\n",
    "            as_ylo = true_info_dict[\"true_as_ylo\"] # str\n",
    "            number = true_info_dict[\"true_number\"] # str\n",
    "            source = true_info_dict[\"true_source\"] # str\n",
    "            bot.send_message(message.chat.id, f\"非常抱歉造成错误，我们已经记录了错误信息！Corrected Information: keywords={keywords}, fields={final_fields}, as_ylo={as_ylo}, number={number}, source={source}。开始查询，请稍后！\", parse_mode=None)\n",
    "        except ValidationError as e:\n",
    "            print(\"### TrueInfo.parse_raw(response.choices[0].message.content) 出现错误 ###\", e.json())\n",
    "            return\n",
    "\n",
    "        if predicted_field != final_fields: # 如果用户认为final_fields正确，但我们的文本分类模型判断错了，则需要记录一下\n",
    "            csv_file = f'data/data_misclassified_user_requests.csv'\n",
    "            new_row = {'Text': user_request, 'Field_true': final_fields, \"Field_predictedbyModel\": predicted_field}\n",
    "            append_row_to_csv(csv_file, new_row)\n",
    "        if fields != final_fields: # 如果用户认为final_fields正确，但gpt-4o-mini判断错了，则也需要记录一下 - 暂时没写 - 暂时规划：判断错的信息作为prompt返回给gpt-4o-mini\n",
    "            pass\n",
    "\n",
    "        #################################### 爬取 ####################################\n",
    "        df_crawled = crawl_details(source, keywords, final_fields, as_ylo, number)\n",
    "        #################################### 爬取 ####################################\n",
    "        \n",
    "        bot.send_message(message.chat.id, \"查询完成，请等待模型总结查询到的内容。\", parse_mode=None)\n",
    "        ###### 展示搜索到的所有结果 ######\n",
    "        crawled_text = convert_csv_to_text(df_crawled, source)\n",
    "        # print(\"!!!\", crawled_text)\n",
    "        # 将查询的到结果进行简化\n",
    "        if crawled_text == \"网站暂时无法爬取，请稍后！\": # 与 convert_csv_to_text() 中对应\n",
    "            bot.send_message(message.chat.id, \"网站暂时无法爬取，请稍后！\", parse_mode=None)\n",
    "        else:\n",
    "            summarized_text = summarize_and_format_articles(crawled_text, source)\n",
    "            # print(\"@@@\", summarized_text)\n",
    "            # 将summarized_text拆分成最大长度为4096的chunk进行发送\n",
    "            text_chunks = split_text_into_chunks(summarized_text)\n",
    "            for chunk in text_chunks:\n",
    "                bot.send_message(message.chat.id, chunk, parse_mode=\"Markdown\")\n",
    "            sent_msg = bot.send_message(message.chat.id, \"以上是本次全部的查询内容。本次查询的数据储存在临时数据库中，是否需要合并到永久数据库中？\\n合并可用于类似情况的推荐。\", parse_mode=\"Markdown\")\n",
    "            bot.register_next_step_handler(sent_msg, merge_data, final_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196f956-2257-4201-8da1-ec47bc94395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def q_processor(input_string):\n",
    "    # 删除所有标点符号\n",
    "    cleaned_string = re.sub(r'[^\\w\\s]', '', input_string)  # \\w匹配字母数字字符，\\s匹配空白字符\n",
    "    # 将左右的空格转换为'+'\n",
    "    cleaned_string = re.sub(r'\\s+', '+', cleaned_string.strip())  \n",
    "    return cleaned_string\n",
    "\n",
    "from crawler.GoogleScholar import fetch_and_save_results_temp\n",
    "from crawler.CNKI import get_page_text,parse_page_text\n",
    "    \n",
    "def crawl_details(source, keywords, final_fields, as_ylo, number):\n",
    "    temp_save_path = f\"data/data_crawled/database_temporary/{final_fields}.csv\"  # 保存路径\n",
    "    q = q_processor(keywords)\n",
    "    if source == \"Google_Scholar\":\n",
    "        start = int(number) // 10 # 判断 number 是否可以被 10 整除\n",
    "        if int(number) % 10 != 0:\n",
    "            start += 1\n",
    "        # print(\"###\", q, as_ylo, start)\n",
    "        df_crawled = fetch_and_save_results_temp(temp_save_path, q, int(as_ylo), start)\n",
    "        # print(\"###\", df_crawled)\n",
    "        return df_crawled\n",
    "    elif source == \"CNKI\":\n",
    "        num_pages = int(number) // 20 + 1 # number除以20的整数部分 + 1\n",
    "        page_text = get_page_text(\"http://search.cnki.com.cn/Search/ListResult\", q, num_pages)\n",
    "        df_crawled = parse_page_text(page_text, int(number), temp_save_path)\n",
    "        # print(\"###\", df_crawled)\n",
    "        return df_crawled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb2dc3-e192-4e0d-8038-205248d2a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_csv_to_text(df, source):\n",
    "    if df is None or df.empty:\n",
    "        return \"网站暂时无法爬取，请稍后！\"\n",
    "\n",
    "    total_rows = len(df)\n",
    "    articles = []\n",
    "\n",
    "    for i in range(total_rows):\n",
    "        article = f\"搜索到的第{i+1}篇文章：\\n\"\n",
    "\n",
    "        if source == \"Google_Scholar\":\n",
    "            title = df.get('Title', pd.Series([None]*total_rows)).iloc[i]\n",
    "            href = df.get('href', pd.Series([None]*total_rows)).iloc[i]\n",
    "            basic_info = df.get('basic_info', pd.Series([None]*total_rows)).iloc[i]\n",
    "            concise_abstract = df.get('Abstract', pd.Series([None]*total_rows)).iloc[i]\n",
    "            full_abstract = df.get('full_abstract', pd.Series([None]*total_rows)).iloc[i]\n",
    "\n",
    "            # 按需添加非空字段\n",
    "            if pd.notna(title):\n",
    "                article += f\"Title: {title}\\n\"\n",
    "            if pd.notna(href):\n",
    "                article += f\"Href: {href}\\n\"\n",
    "            if pd.notna(basic_info):\n",
    "                article += f\"Basic Information: {basic_info}\\n\"\n",
    "            if pd.notna(concise_abstract):\n",
    "                article += f\"Consise Abstract: {concise_abstract}\\n\"\n",
    "            if pd.notna(full_abstract) and full_abstract != 'N/A':\n",
    "                article += f\"Full Abstract: {full_abstract}\\n\"\n",
    "\n",
    "        elif source == \"CNKI\":\n",
    "            title = df.get('Title', pd.Series([None]*total_rows)).iloc[i]\n",
    "            author = df.get('Author', pd.Series([None]*total_rows)).iloc[i]\n",
    "            paper_source = df.get('Source', pd.Series([None]*total_rows)).iloc[i]\n",
    "            paper_type = df.get('Type', pd.Series([None]*total_rows)).iloc[i]\n",
    "            date = df.get('Date', pd.Series([None]*total_rows)).iloc[i]\n",
    "            abstract = df.get('Abstract', pd.Series([None]*total_rows)).iloc[i]\n",
    "            keywords = df.get('Keywords', pd.Series([None]*total_rows)).iloc[i]\n",
    "            download = df.get('Download', pd.Series([None]*total_rows)).iloc[i]\n",
    "            citations = df.get('Citations', pd.Series([None]*total_rows)).iloc[i]\n",
    "            href = df.get('href', pd.Series([None]*total_rows)).iloc[i]\n",
    "\n",
    "            # 按需添加非空字段\n",
    "            if pd.notna(title):\n",
    "                article += f\"Title: {title}\\n\"\n",
    "            if pd.notna(author):\n",
    "                article += f\"Author: {author}\\n\"\n",
    "            if pd.notna(paper_source):\n",
    "                article += f\"Source: {paper_source}\\n\"\n",
    "            if pd.notna(paper_type):\n",
    "                article += f\"Type: {paper_type}\\n\"\n",
    "            if pd.notna(date):\n",
    "                article += f\"Date: {date}\\n\"\n",
    "            if pd.notna(abstract):\n",
    "                article += f\"Abstract: {abstract}\\n\"\n",
    "            if pd.notna(keywords):\n",
    "                article += f\"Keywords: {keywords}\\n\"\n",
    "            if pd.notna(download):\n",
    "                article += f\"Download: {download}\\n\"\n",
    "            if pd.notna(citations):\n",
    "                article += f\"Citations: {citations}\\n\"\n",
    "            if pd.notna(href):\n",
    "                article += f\"Link: {href}\\n\"\n",
    "\n",
    "        # 确保每篇文章都输出到控制台\n",
    "        # print(\"::::::\", article)\n",
    "\n",
    "        # 将每篇文章添加到列表\n",
    "        articles.append(article)\n",
    "\n",
    "    # 将所有文章内容合并并返回\n",
    "    return \"\\n\".join(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f0e36-edf9-40b0-a750-487052dd47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_format_articles(text, source):\n",
    "    if source == \"Google_Scholar\":\n",
    "        system_message_content = (\n",
    "            \"You are an expert in summarizing academic articles. The user will provide you with details of several articles including title, href, \"\n",
    "            \"basic information(including authors, year, journal), and abstract(consise abstract is always available, but the full abstract may not be available). \"\n",
    "            \"Your task is to generate a concise summary of each article, focusing on key details and simplifying the abstract where possible. \"\n",
    "            \"Format the output in the following manner:\\n\\n\"\n",
    "            \"1. **Title of the Article**\\n\"\n",
    "            \"   - **Author:** Author's Name\\n\"\n",
    "            \"   - **Year:** Year of Publication\\n\"\n",
    "            \"   - **Journal:** Journal Name\\n\"\n",
    "            \"   - **Href:** href\\n\"\n",
    "            \"   - **Summary:** Simplified abstract or summary of the article.\\n\\n\"\n",
    "            \"Ensure each article follows this structure, and keep the summaries brief and clear.\"\n",
    "        )\n",
    "    elif source == \"CNKI\":\n",
    "        system_message_content = (\n",
    "            \"You are an expert in summarizing academic articles from CNKI. The user will provide you with details of several articles including title, author, source (journal), \"\n",
    "            \"type of the article, date of publication, abstract, keywords, download and citation counts, and link. Your task is to generate a concise summary of each article, \"\n",
    "            \"focusing on key details and simplifying the abstract where possible. Format the output in the following manner:\\n\\n\"\n",
    "            \"1. **Title of the Article**\\n\"\n",
    "            \"   - **Author:** Author's Name\\n\"\n",
    "            \"   - **Source (Journal):** Journal Name\\n\"\n",
    "            \"   - **Type:** Type of the article\\n\"\n",
    "            \"   - **Date:** Date of Publication\\n\"\n",
    "            \"   - **Keywords:** Keywords associated with the article\\n\"\n",
    "            \"   - **Download Count:** Number of downloads\\n\"\n",
    "            \"   - **Citation Count:** Number of citations\\n\"\n",
    "            \"   - **Link:** Article link\\n\"\n",
    "            \"   - **Summary:** Simplified abstract or summary of the article.\\n\\n\"\n",
    "            \"Ensure each article follows this structure, and keep the summaries brief and clear.\"\n",
    "        )\n",
    "    else:\n",
    "        return \"Invalid source specified.\"\n",
    "\n",
    "    # Construct messages based on the source\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message_content},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    \n",
    "    # Call the API to summarize and format the articles\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    summary = response.choices[0].message.content\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37eff15-9414-442d-83e7-6eb94c508b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解决 Bad Request: message is too long 的问题 - Telegram 发送消息时的字符限制是 4096 个字符\n",
    "def split_text_into_chunks(text, chunk_size=4096):\n",
    "    chunks = []\n",
    "    \n",
    "    while len(text) > chunk_size:\n",
    "        # 尝试在chunk_size之前找到最后一个空行位置\n",
    "        split_index = text[:chunk_size].rfind('\\n\\n')\n",
    "        \n",
    "        # 如果没有找到空行，则回退到最后一个单独换行符处\n",
    "        if split_index == -1:\n",
    "            split_index = text[:chunk_size].rfind('\\n')\n",
    "        \n",
    "        # 如果找不到单独的换行符，默认拆分点为chunk_size\n",
    "        if split_index == -1:\n",
    "            split_index = chunk_size\n",
    "        \n",
    "        chunks.append(text[:split_index].strip())  # 去除末尾多余的空格\n",
    "        text = text[split_index:].strip()  # 去除开头多余的空格\n",
    "    \n",
    "    # 添加最后的剩余部分\n",
    "    chunks.append(text.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8374519-e0e5-4952-8d2d-daa698b1eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "# 定义请求格式\n",
    "response_format_merge_request_schema = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"merge_request_schema\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"merge_to_permanent\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Indicates whether the user wants to merge the temporary database into the permanent database.\",\n",
    "                    \"enum\": [True, False]\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"merge_to_permanent\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# 定义 Pydantic 模型\n",
    "class MergeRequest(BaseModel):\n",
    "    merge_to_permanent: bool\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 合并数据的函数\n",
    "def merge_data(message, final_fields):\n",
    "    message_text = message.text  # 类似：1.是的需要合并；2.不需要合并\n",
    "\n",
    "    messages_to_model = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in database management. The user has a temporary database, and your task is to analyze the user's request and determine whether they want to merge this temporary database into the permanent database.\"},\n",
    "        {\"role\": \"user\", \"content\": message_text}\n",
    "    ]\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-mini-2024-07-18\", messages=messages_to_model, response_format=response_format_merge_request_schema)\n",
    "\n",
    "    try:\n",
    "        merge_request = MergeRequest.parse_raw(response.choices[0].message.content)\n",
    "        merge_request_dict = merge_request.dict()\n",
    "        merge_to_permanent = merge_request_dict[\"merge_to_permanent\"]\n",
    "    except ValidationError as e:\n",
    "        print(\"### MergeRequest.parse_raw(response.choices[0].message.content) 出现错误 ###\", e.json())\n",
    "        return\n",
    "\n",
    "    if merge_to_permanent:  # 需要合并\n",
    "        temp_file_path = f\"data/data_crawled/database_temporary/{final_fields}.csv\"\n",
    "        perm_file_path = f\"data/data_crawled/database_persistent/{final_fields}.csv\"\n",
    "\n",
    "        # 检查临时数据库中的文件是否存在\n",
    "        if not os.path.exists(temp_file_path):\n",
    "            print(f\"### 临时数据库中的文件 {temp_file_path} 不存在 ###\")\n",
    "            return\n",
    "\n",
    "        # 读取临时数据库中的 CSV 文件\n",
    "        temp_df = pd.read_csv(temp_file_path)\n",
    "\n",
    "        # 如果永久数据库中没有对应的 CSV 文件，则直接保存\n",
    "        if not os.path.exists(perm_file_path):\n",
    "            temp_df.to_csv(perm_file_path, index=False)\n",
    "            print(f\"### {final_fields}.csv 已合并到永久数据库 ###\")\n",
    "        else:\n",
    "            # 读取永久数据库中的 CSV 文件\n",
    "            perm_df = pd.read_csv(perm_file_path)\n",
    "\n",
    "            # 获取所有的列名（包括 Google Scholar 和 CNKI 的列）\n",
    "            all_columns = list(set(temp_df.columns).union(set(perm_df.columns)))\n",
    "\n",
    "            # 将缺失的列填充为 None，保证两个数据框的列一致\n",
    "            for column in all_columns:\n",
    "                if column not in temp_df.columns:\n",
    "                    temp_df[column] = None\n",
    "                if column not in perm_df.columns:\n",
    "                    perm_df[column] = None\n",
    "\n",
    "            # 进行合并并去重\n",
    "            merged_df = pd.concat([perm_df, temp_df], ignore_index=True)\n",
    "            merged_df.drop_duplicates(inplace=True)\n",
    "\n",
    "            # 保存合并后的数据到永久数据库\n",
    "            merged_df.to_csv(perm_file_path, index=False)\n",
    "            print(f\"### {final_fields}.csv 已与永久数据库中的现有文件合并 ###\")\n",
    "\n",
    "        print(\"### 数据集已成功合并到永久数据库 ###\")\n",
    "        bot.send_message(message.chat.id, \"合并完成！\", parse_mode=None)\n",
    "    else:\n",
    "        print(\"### 用户选择不合并临时数据库 ###\")\n",
    "        bot.send_message(message.chat.id, \"好的，本次查询内容将不会被合并。\", parse_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79432174-1829-4670-aba4-1b0b84f96ce7",
   "metadata": {},
   "source": [
    "# /recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527cad0-7472-48fe-895b-d7cdc8f063cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# 假设你已经加载了 df，包含 'Abstract', 'Title', 和 'Href' 列\n",
    "\n",
    "# 1. 生成嵌入\n",
    "def create_embeddings(text):\n",
    "    response = client.embeddings.create(input=text, model='text-embedding-3-large')  # 使用适当的模型\n",
    "    embeddings = response.data[0].embedding\n",
    "    return embeddings\n",
    "\n",
    "# 生成嵌入并存入df新的一列 'Abstract embeddings'\n",
    "def generate_embeddings_for_dataframe(df, file_path):\n",
    "    # 检查 'Abstract embeddings' 列是否存在，如果不存在则创建\n",
    "    if 'Abstract embeddings' not in df.columns:\n",
    "        df['Abstract embeddings'] = None\n",
    "\n",
    "    embeddings_updated = False\n",
    "\n",
    "    # 遍历 'Abstract' 列，只有当 'Abstract embeddings' 为空时才生成嵌入\n",
    "    for i, abstract in df['Abstract'].items():  # 修改为 items()\n",
    "        if pd.isna(df.at[i, 'Abstract embeddings']):\n",
    "            df.at[i, 'Abstract embeddings'] = create_embeddings(abstract)\n",
    "            embeddings_updated = True  # 标记更新\n",
    "\n",
    "    # 仅当更新了嵌入时才保存到原文件，避免不必要的I/O操作\n",
    "    if embeddings_updated:\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"### 已更新嵌入并保存到 {file_path} ###\")\n",
    "    else:\n",
    "        print(f\"### 没有新的嵌入需要更新，跳过保存 ###\")\n",
    "\n",
    "    return df\n",
    "\n",
    "import ast  # 用于将字符串转化为列表\n",
    "\n",
    "# 处理抽象嵌入列表，确保每个嵌入都是list而非str\n",
    "def process_abstract_embeddings(abstract_embeddings):\n",
    "    processed_embeddings = []\n",
    "    for embedding in abstract_embeddings:\n",
    "        if isinstance(embedding, str):  # 如果嵌入是字符串格式\n",
    "            try:\n",
    "                embedding = ast.literal_eval(embedding)  # 将字符串转为列表\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"无法解析嵌入：{embedding}\")\n",
    "                continue\n",
    "        processed_embeddings.append(embedding)\n",
    "    return processed_embeddings\n",
    "\n",
    "# 2. 根据用户请求生成嵌入并找到最相似的Top_k论文\n",
    "def find_similar_papers(request_vector, df, top_k=5):\n",
    "    abstract_embeddings = df['Abstract embeddings'].tolist()\n",
    "    \n",
    "    # 确保所有嵌入是正确的数值列表\n",
    "    abstract_embeddings = process_abstract_embeddings(abstract_embeddings)\n",
    "    \n",
    "    # print(type(abstract_embeddings), type(abstract_embeddings[0]), len(abstract_embeddings)) # 第一次输出:<class 'list'> <class 'list'> 10,第二次输出:<class 'list'> <class 'str'> 10\n",
    "    \n",
    "    # 使用 NearestNeighbors 查找最相似的向量\n",
    "    nbrs = NearestNeighbors(n_neighbors=top_k, algorithm='ball_tree').fit(abstract_embeddings)\n",
    "    \n",
    "    distances, indices = nbrs.kneighbors([request_vector])\n",
    "    \n",
    "    # 收集最相似的论文 title 和 href\n",
    "    similar_papers = []\n",
    "    for index, distance in zip(indices[0], distances[0]):\n",
    "        similar_papers.append({\n",
    "            'Title': df['Title'].iloc[index],\n",
    "            'Href': df['href'].iloc[index],\n",
    "            'Distance': distance\n",
    "        })\n",
    "    \n",
    "    return similar_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71db817-9b7c-460c-bc36-d3cf9fbca008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 实现推荐功能\n",
    "@bot.message_handler(commands=['recommend'])\n",
    "def recommend_initialize(message):\n",
    "    \"\"\"\n",
    "    让用户选择领域并提问。\n",
    "    \"\"\"\n",
    "    markup = telebot.types.ReplyKeyboardMarkup(one_time_keyboard=True) # 卧槽,牛逼啊,变成选择题了!\n",
    "    fields = ['Education', 'Computer_Science', 'Medicine', 'Literature', 'Other']\n",
    "    for field in fields:\n",
    "        markup.add(field)\n",
    "    bot.send_message(message.chat.id, \"请选择一个领域:\", reply_markup=markup)\n",
    "    bot.register_next_step_handler(message, handle_fields)\n",
    "\n",
    "def handle_fields(message):\n",
    "    selected_field = message.text # 例如:Education\n",
    "    file_path = f\"data/data_crawled/database_persistent/{selected_field}.csv\"\n",
    "    bot.send_message(message.chat.id, \"您需要我推荐什么文章?\")\n",
    "    bot.register_next_step_handler(message, recommend_papers, file_path, top_k=3)\n",
    "\n",
    "def recommend_papers(message, file_path, top_k=5):\n",
    "    request = message.text\n",
    "    df = pd.read_csv(file_path)\n",
    "    # 为用户请求生成嵌入\n",
    "    request_embedding = create_embeddings(request)\n",
    "\n",
    "    # 更新 DataFrame 中的嵌入并保存到文件\n",
    "    df = generate_embeddings_for_dataframe(df, file_path)\n",
    "\n",
    "    # 找到与用户请求最相似的论文\n",
    "    similar_papers = find_similar_papers(request_embedding, df, top_k=top_k)\n",
    "    \n",
    "    # 组合推荐结果\n",
    "    total_content = \"Here are the top recommended papers based on your request:\\n\\n\"\n",
    "    for paper in similar_papers:\n",
    "        total_content += f\"Title: {paper['Title']}\\n\"\n",
    "        total_content += f\"Link: {paper['Href']}\\n\"\n",
    "        total_content += f\"Similarity Score: {paper['Distance']}\\n\\n\"\n",
    "    \n",
    "    # 使用 OpenAI 生成完整的推荐响应（可选）\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant that helps with academic paper recommendations.\"},\n",
    "        {\"role\": \"user\", \"content\": total_content}\n",
    "    ]\n",
    "\n",
    "    # 生成回复\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    bot.send_message(message.chat.id, completion.choices[0].message.content, parse_mode=\"Markdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a3391-91ba-479e-b5c6-f0328a2f316e",
   "metadata": {},
   "source": [
    "# /news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d66a0-87eb-4168-adee-4f05df6d8aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@bot.message_handler(commands=['news'])\n",
    "def initiate_query_news(message):\n",
    "    text = \"您需要查询什么最新的新闻？\"\n",
    "    sent_msg = bot.send_message(message.chat.id, text, parse_mode=\"Markdown\")\n",
    "    bot.register_next_step_handler(sent_msg, process_news_query)  # 修改函数名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756c754-cee7-4aa1-96d2-6e836bda5e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def query_perplexity(user_request):\n",
    "    url = \"https://api.perplexity.ai/chat/completions\"\n",
    "    \n",
    "    # 在用户请求后追加一句话请求返回来源\n",
    "    user_request_with_source = user_request + \" Please include the source URL of the news.\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"llama-3.1-sonar-small-128k-online\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Provide a concise and factual response, with clear and precise information.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_request_with_source  # 动态使用用户请求，并请求来源\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 4000,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"return_citations\": True,  # 启用返回引用\n",
    "        \"search_domain_filter\": [\"perplexity.ai\"],\n",
    "        \"return_images\": False,\n",
    "        \"return_related_questions\": False,\n",
    "        \"search_recency_filter\": \"month\",\n",
    "        \"top_k\": 0,\n",
    "        \"stream\": False,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 1\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer <token>\",  # 替换为你的token\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "    \n",
    "    # 检查响应状态\n",
    "    if response.status_code == 200:\n",
    "        data = json.loads(response.text)\n",
    "        content = data['choices'][0]['message']['content']\n",
    "        return content  # 返回API的回答\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\"  # 返回错误信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b31d59-14b7-423b-b9fe-5df92563e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改函数名称，并处理输入请求\n",
    "def process_news_query(message): \n",
    "    user_request = message.text\n",
    "    perplexity_answer = query_perplexity(user_request)\n",
    "    bot.send_message(message.chat.id, perplexity_answer, parse_mode=\"Markdown\")\n",
    "    end_message = \"本次查询结束。如想重新查询，请输入“/query_news”。\"\n",
    "    bot.send_message(message.chat.id, end_message, parse_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb033c38-5267-445f-a5d1-6fe7e78f2854",
   "metadata": {},
   "source": [
    "# /upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59a04e-0968-41c1-84e7-466ed972ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保保存PDF文件的文件夹存在\n",
    "base_dir = 'rawPDFs/'\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "# 确保所有类别的文件夹存在\n",
    "fields = [\"Education\", \"Computer_Science\", \"Medicine\", \"Literature\", \"Other\"]\n",
    "for field in fields:\n",
    "    field_path = os.path.join(base_dir, field)\n",
    "    if not os.path.exists(field_path):\n",
    "        os.makedirs(field_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ddf01-2623-45d5-bb1f-0bb4779562ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理命令上传文件并指定类别\n",
    "@bot.message_handler(commands=['upload'])\n",
    "def handle_upload_command(message):\n",
    "    # text = \"您好！请上传您的文档，并在'添加说明'中显式说明您的文档属于 Eduction, Computer_Science, Medicine, Literature, Other 这五个领域中的哪一个？\\n注意：请单个单个上传文档！\"\n",
    "    # sent_msg = bot.send_message(message.chat.id, text, parse_mode=\"Markdown\")\n",
    "    text = \"您好！请上传您的文档，并在“添加说明”中显式说明您的文档属于 Education, Computer_Science, Medicine, Literature, Other 这五个领域中的哪一个？另外，请单个单个上传文档！\"\n",
    "    sent_msg = bot.send_message(message.chat.id, text, parse_mode=None)\n",
    "    bot.register_next_step_handler(sent_msg, handle_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13321ff-3854-4b98-be44-61c76d6b1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "response_format_extract_fields = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"response_format_extract_fields\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"fields\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"从用户请求中判断用户的文档属于哪一个领域。例子: User request: 我的文档属于计算机领域。 - Extracted fields: Computer_Science\",\n",
    "                    \"enum\": [\"Education\", \"Computer_Science\", \"Medicine\", \"Literature\", \"Other\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"fields\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "class Fields(BaseModel):\n",
    "    fields: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14600503-69b6-43a6-8d72-a97410375e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=api_key, base_url=api_base) # 使用第三方低价API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c9d79-63de-4d45-ae9b-2b9baa739312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader # 处理单个PDF\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader # 处理文件夹下的所有PDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "@bot.message_handler(content_types=['document'])\n",
    "def handle_document(message):\n",
    "    if message.caption:  # 用户可能在上传文件时指定了类别\n",
    "        fields_message = message.caption.strip()\n",
    "        # print(\"######\", fields_message, \"######\")\n",
    "        messages_to_model=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert at structured data extraction. You will be given unstructured text from a user request and should convert it into the given structure.\"},\n",
    "            {\"role\": \"user\", \"content\": fields_message}\n",
    "        ]\n",
    "        response = client.chat.completions.create(model=\"gpt-4o-mini-2024-07-18\", messages=messages_to_model, response_format=response_format_extract_fields)\n",
    "    \n",
    "        try:\n",
    "            fields = Fields.parse_raw(response.choices[0].message.content)\n",
    "            fields_dict = fields.dict()\n",
    "            fields = fields_dict[\"fields\"]\n",
    "        except ValidationError as e:\n",
    "            print(\"### Fields.parse_raw(response.choices[0].message.content) 出现错误 ###\", e.json())\n",
    "            \n",
    "    else: # 如果没有在“添加说明”中显式地指出类别，则默认分到Other\n",
    "        fields = \"Other\"\n",
    "\n",
    "    # 保存文档到对应类别文件夹\n",
    "    file_info = bot.get_file(message.document.file_id)\n",
    "    downloaded_file = bot.download_file(file_info.file_path)\n",
    "    save_path = os.path.join(base_dir, fields, message.document.file_name)\n",
    "    \n",
    "    with open(save_path, 'wb') as new_file:\n",
    "        new_file.write(downloaded_file)\n",
    "    \n",
    "    bot.reply_to(message, f\"文档已保存到 {save_path}！请稍后!我们将切分文档,并将文档上传到Pinecone矢量数据库.\")\n",
    "\n",
    "    # 保存完到本地之后,先处理文档,再将文件上传到Pinecone矢量数据库\n",
    "    file_path = save_path # 指定要加载的 PDF 文件路径\n",
    "    loader = PyPDFLoader(file_path) # 创建 PyPDFLoader 实例\n",
    "    documents = loader.load() # 加载 PDF 文件并转换为文本数据\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap  = 50)\n",
    "    documents_chunks = splitter.split_documents(documents)\n",
    "\n",
    "    namespace = fields\n",
    "    vector_store = PineconeVectorStore(index=index, namespace=namespace, embedding=embeddings)\n",
    "    uuids = [str(uuid4()) for _ in range(len(documents_chunks))]\n",
    "    vector_store.add_documents(documents=documents_chunks, ids=uuids)\n",
    "\n",
    "    bot.reply_to(message, f\"文档已经被切分且上传到Pinecone矢量数据库!本次操作结束.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c6b58-61f6-456f-8935-a0075c7b0403",
   "metadata": {},
   "source": [
    "# /rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa82b7-043a-42e6-a572-d0435d251345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=api_key, base_url=api_base) # 使用第三方低价API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b61a4b-8d68-4e04-9167-ac3dd38ad532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户输入命令后选择文档类别\n",
    "@bot.message_handler(commands=['rag'])\n",
    "def ask_field(message):\n",
    "    \"\"\"\n",
    "    让用户选择领域并提问。\n",
    "    \"\"\"\n",
    "    markup = telebot.types.ReplyKeyboardMarkup(one_time_keyboard=True) # 卧槽,牛逼啊,变成选择题了!\n",
    "    fields = ['Education', 'Computer_Science', 'Medicine', 'Literature', 'Other']\n",
    "    for field in fields:\n",
    "        markup.add(field)\n",
    "    bot.send_message(message.chat.id, \"请选择一个领域:\", reply_markup=markup)\n",
    "    bot.register_next_step_handler(message, get_user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a6c6f-9027-4a9f-a16c-c5a96ffd4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_question(message):\n",
    "    \"\"\"\n",
    "    获取用户的问题并开始处理查询。\n",
    "    \"\"\"\n",
    "    selected_field = message.text\n",
    "    bot.send_message(message.chat.id, \"请输入您的问题:\")\n",
    "    bot.register_next_step_handler(message, lambda m: process_query(m, selected_field))\n",
    "\n",
    "def process_query(message, field):\n",
    "    \"\"\"\n",
    "    处理用户查询：在Pinecone中搜索相似内容并生成答案。\n",
    "    \"\"\"\n",
    "    user_question = message.text\n",
    "    namespace = field\n",
    "    results = search_similar_documents(user_question, namespace)\n",
    "    top_k_results = [res.page_content for res, _ in results]\n",
    "    \n",
    "    # 将用户问题与检索结果一同传给OpenAI生成最终答案\n",
    "    final_answer = generate_answer(user_question, top_k_results)\n",
    "    \n",
    "    # 返回结果给用户\n",
    "    bot.send_message(message.chat.id, final_answer, parse_mode=\"Markdown\")\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "def search_similar_documents(query, namespace, top_k=5):\n",
    "    \"\"\"\n",
    "    在Pinecone的特定namespace中查找与query最相似的文档内容。\n",
    "    \"\"\"\n",
    "    vector_store = PineconeVectorStore(index=index, namespace=namespace, embedding=embeddings)\n",
    "    results = vector_store.similarity_search_with_score(query, k=top_k)\n",
    "    return results\n",
    "\n",
    "def generate_answer(user_question, documents):\n",
    "    \"\"\"\n",
    "    将用户问题和检索到的文档内容传入OpenAI模型生成答案。\n",
    "    \"\"\"\n",
    "    context = \"\\n\".join(documents)\n",
    "    user_prompt = f\"用户问题: {user_question}\\n相关文档:\\n{context}\\n请根据这些内容回答问题：\"\n",
    "\n",
    "    # system prompt 用于指导模型回答\n",
    "    system_prompt = (\n",
    "        \"你是一个帮助用户回答基于文档内容的问题的助手。\"\n",
    "        \"当用户提出问题时，你应该根据提供的相关文档中的信息进行回答。\"\n",
    "        \"如果文档中包含答案，尽可能引用文档内容并确保准确性。\"\n",
    "        \"如果文档中没有明确的答案，请基于你对文档的理解提供推断，但避免过度猜测。\"\n",
    "    )\n",
    "    \n",
    "    messages_to_model = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    # 使用OpenAI生成最终答案\n",
    "    completion = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages_to_model)\n",
    "    # print(user_prompt)\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d67ac-de52-47eb-8445-b52cf867579b",
   "metadata": {},
   "source": [
    "# 非命令消息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f7db86-1fb2-41bc-a623-04aef61dc849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 处理所有非命令的消息\n",
    "@bot.message_handler(func=lambda message: not message.text.startswith('/'))\n",
    "def echo_all(message):\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  \n",
    "                {\"role\": \"user\", \"content\": message.text}\n",
    "            ]\n",
    "        )\n",
    "        bot_reply = completion.choices[0].message.content\n",
    "        bot.reply_to(message, bot_reply)\n",
    "\n",
    "    except Exception as e:\n",
    "        bot.reply_to(message, \"出错了，请稍后再试！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09f31f-4bdc-4361-8bb9-1a6d2cf855c2",
   "metadata": {},
   "source": [
    "# 启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb24d5f-c01d-4daf-a9a1-c717b1f3d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启动Bot\n",
    "# bot.infinity_polling()\n",
    "bot.polling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247ec71-fcde-4df1-ad40-6e06f1bcf3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7fb49f-4860-443a-952a-3ff1f27edbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6cb689-6376-4212-86ce-02ef1b3a9bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
