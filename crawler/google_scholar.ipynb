{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e266ebaa-75ad-42c9-a6aa-5b975eadab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"]=\"127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"]=\"127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e10a450-47c1-465c-b472-9cccff3ac295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5906cf-ac92-4916-8b92-3fc5afda2850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "def fetch_google_scholar_results(inquiry, as_ylo, start, hl=\"zh-CN\"):\n",
    "    base_url = \"https://scholar.google.com/scholar\"\n",
    "    query_params = {\n",
    "        \"q\": inquiry,\n",
    "        \"hl\": hl,\n",
    "        \"as_sdt\": \"0,5\",\n",
    "        \"as_ylo\": as_ylo,\n",
    "        \"start\": start\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    response = requests.get(base_url, params=query_params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        divs = soup.find_all(\"div\", class_=\"gs_r gs_or gs_scl\")\n",
    "\n",
    "        for div in divs:\n",
    "            result = {}\n",
    "            h3 = div.find(\"h3\")\n",
    "            if h3:\n",
    "                a = h3.find(\"a\")\n",
    "                if a:\n",
    "                    title = a.get_text()\n",
    "                    result[\"Title\"] = title\n",
    "                    href = a.get('href')\n",
    "                    result[\"href\"] = href\n",
    "                    \n",
    "            gs_a = div.find(\"div\", class_=\"gs_a\")\n",
    "            if gs_a:\n",
    "                basic_info = gs_a.get_text()\n",
    "                result[\"basic_info\"] = basic_info\n",
    "\n",
    "            gs_rs = div.find(\"div\", class_=\"gs_rs\")\n",
    "            if gs_rs:\n",
    "                abstract = gs_rs.get_text()\n",
    "                result[\"concise_abstract\"] = abstract\n",
    "\n",
    "            gs_or_nvi = div.find(\"a\", class_=\"gs_or_nvi\")\n",
    "            if gs_or_nvi:\n",
    "                snapshot = gs_or_nvi.get('href')\n",
    "                if snapshot and snapshot != \"javascript:void(0)\":\n",
    "                    try:\n",
    "                        second_response = requests.get(snapshot)\n",
    "                        second_response.raise_for_status()  # Raise an exception for bad status codes\n",
    "                    except requests.exceptions.InvalidSchema:\n",
    "                        # 如果是无效的URL架构，可能是相对路径，将其转换为绝对路径\n",
    "                        snapshot_url = urljoin(\"https://scholar.google.com\", snapshot)\n",
    "                        try:\n",
    "                            second_response = requests.get(snapshot_url)\n",
    "                            second_response.raise_for_status()\n",
    "                        except requests.exceptions.RequestException as e:\n",
    "                            print(f\"Error occurred while fetching secondary URL (after conversion): {e}\")\n",
    "                            continue\n",
    "                    except requests.exceptions.RequestException as e:\n",
    "                        print(f\"Error occurred while fetching secondary URL: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    if second_response.status_code == 200:\n",
    "                        second_soup = BeautifulSoup(second_response.content, \"html.parser\")\n",
    "                        article_abstract_div = second_soup.find(\"div\", id=\"articleAbstract\")\n",
    "                        if article_abstract_div is not None: \n",
    "                            articleAbstract = article_abstract_div.get_text()\n",
    "                            result[\"full_abstract\"] = articleAbstract\n",
    "\n",
    "            results.append(result)\n",
    "    elif response.status_code == 429:\n",
    "        retry_after = int(response.headers.get(\"Retry-After\", 1))\n",
    "        print(f\"Too many requests. Retrying after {retry_after} seconds...\")\n",
    "    else:\n",
    "        print(\"Error\", response.status_code)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d11a1c-af8f-4af9-a22d-7d5391a74820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many requests. Retrying after 1 seconds...\n",
      "Already got 0 results\n"
     ]
    }
   ],
   "source": [
    "q = \"educationalpsychologydefinition\"\n",
    "as_ylo = 2022\n",
    "\n",
    "# Store results for all pages\n",
    "all_results = []\n",
    "for i in range(0, 10, 10):  # 110 represents the number of pages to be crawled, adding 10 search results at a time\n",
    "    results = fetch_google_scholar_results(q, as_ylo, i)\n",
    "    all_results.extend(results)\n",
    "    time.sleep(40) # Delays are added to avoid frequent requests\n",
    "    print(\"Already got {} results\".format(len(all_results)))\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# # Use regular expressions to extract the date and the content after the date and save it as a new column\n",
    "# date_publisher_pattern = r'(\\d{4})(.*)'  # Add a capture group, with the date and what comes after the date in parentheses\n",
    "# extracted = df['Authors'].str.extract(date_publisher_pattern)\n",
    "\n",
    "# # Assign the extracted results to a new column\n",
    "# df['Date'] = extracted[0]  # date column\n",
    "# df['Publisher'] = extracted[1].str.strip()  # Remove first and last Spaces for content after the date and save as publisher column\n",
    "\n",
    "# # Delete the date and publisher in the Authors column\n",
    "# df['Authors'] = df['Authors'].str.replace(date_publisher_pattern, '', regex=True).str.strip()\n",
    "\n",
    "# # Write the DataFrame to an Excel file\n",
    "# df.to_excel(\"google_scholar_results_with_date_publishers.xlsx\", index=False)\n",
    "\n",
    "# # Output all results\n",
    "# for result in all_results:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2283b4b7-c9f5-4fa4-ac95-8da3c2737a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basic_info</th>\n",
       "      <th>concise_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNS Page - Language</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            basic_info concise_abstract\n",
       "0  SNS Page - Language                 "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3ca81-cb11-4385-b716-51e3ee940e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def make_request_with_retry(url, max_retries=5):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 429:\n",
    "            retry_after = int(response.headers.get(\"Retry-After\", 1))\n",
    "            print(f\"Too many requests. Retrying after {retry_after} seconds...\")\n",
    "            time.sleep(retry_after)\n",
    "            retries += 1\n",
    "        else:\n",
    "            return response\n",
    "    raise Exception(\"Max retries exceeded\")\n",
    "\n",
    "url = \"http://example.com\"\n",
    "response = make_request_with_retry(url)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c81d68-f8b7-4e90-ae31-82c563562ac0",
   "metadata": {},
   "source": [
    "# 数据持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af3617-8b93-402b-b448-e9e294fa286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义持久化文件路径\n",
    "persist_file = 'data/data_crawled/googel_scholar.csv'\n",
    "\n",
    "# 初始化持久化存储\n",
    "if not os.path.exists(persist_file):\n",
    "    # 如果持久化文件不存在，创建一个空的 DataFrame 并保存\n",
    "    df_persist = pd.DataFrame(columns=['Title', 'href', 'basic_info', 'concise_abstract', 'full_abstract'])\n",
    "    df_persist.to_csv(persist_file, index=False)\n",
    "else:\n",
    "    # 如果持久化文件存在，读取文件内容\n",
    "    df_persist = pd.read_csv(persist_file)\n",
    "\n",
    "#######################################################################################\n",
    "q = \"全球胜任力培养\"\n",
    "as_ylo = 2020\n",
    "\n",
    "# Store results for all pages\n",
    "all_results = []\n",
    "for i in range(0, 10, 10):  # 110 represents the number of pages to be crawled, adding 10 search results at a time\n",
    "    results = fetch_google_scholar_results(q, as_ylo, i)\n",
    "    all_results.extend(results)\n",
    "    # time.sleep(40) # Delays are added to avoid frequent requests\n",
    "    print(\"Already got {} results\".format(len(all_results)))\n",
    "\n",
    "df_new_results = pd.DataFrame(all_results)\n",
    "df_new_results = df_new_results.dropna(subset=['Title']) # 删除Title列为NaN的行\n",
    "#######################################################################################\n",
    "\n",
    "# 将新爬取的结果转换为 DataFrame\n",
    "# df_new_results = pd.DataFrame(new_results)\n",
    "\n",
    "# 比较并筛选未爬取过的数据\n",
    "df_combined = pd.concat([df_persist, df_new_results]).drop_duplicates(subset=['Title'], keep=False)\n",
    "df_new_unique = df_combined[df_combined.index >= len(df_persist)]\n",
    "\n",
    "# 如果有新数据，更新持久化存储\n",
    "if not df_new_unique.empty:\n",
    "    df_persist = pd.concat([df_persist, df_new_unique])\n",
    "    df_persist.to_csv(persist_file, index=False)\n",
    "    print(f\"Added {len(df_new_unique)} new entries to the persistent storage.\")\n",
    "else:\n",
    "    print(\"No new entries to add.\")\n",
    "\n",
    "# 打印更新后的持久化存储内容\n",
    "# print(df_persist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0c9e2-583d-49f6-82b9-7a79542662a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4f746-d162-4ac3-8f06-e8866f4e25d7",
   "metadata": {},
   "source": [
    "# 总代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4cd449-6314-464d-aa3b-00e9c5ad3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义持久化文件路径\n",
    "persist_file = 'google_scholar.csv'\n",
    "\n",
    "# 初始化持久化存储\n",
    "if not os.path.exists(persist_file):\n",
    "    # 如果持久化文件不存在，创建一个空的 DataFrame 并保存\n",
    "    df_persist = pd.DataFrame(columns=['Title', 'href', 'basic_info', 'concise_abstract', 'full_abstract'])\n",
    "    df_persist.to_csv(persist_file, index=False)\n",
    "else:\n",
    "    # 如果持久化文件存在，读取文件内容\n",
    "    df_persist = pd.read_csv(persist_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5f78a-a50b-4553-9bf4-4aee5609804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_persist.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa50826-5cf8-4056-8a9b-1e3b82bed10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_google_scholar_results(q, as_ylo, start, hl=\"zh-CN\"):\n",
    "    global df_persist\n",
    "    \"\"\"\n",
    "    q：查询的内容，如有多个，用“+”连接。\n",
    "    hl：指定界面语言，默认为中文，也可以选择“en”。\n",
    "    as_ylo：指定文章的起始年份。\n",
    "    start：谷歌学术默认一页10篇paper，所以start为10的倍数（考虑0），代表从第几页开始爬取。\n",
    "    \"\"\"\n",
    "    base_url = \"https://scholar.google.com/scholar\"\n",
    "    query_params = {\n",
    "        \"q\": q,\n",
    "        \"hl\": hl,\n",
    "        \"as_sdt\": \"0,5\",\n",
    "        \"as_ylo\": as_ylo,\n",
    "        \"start\": start\n",
    "    }  # hl=zh-CN表示中国大陆的中文；%2C表示逗号；%2B表示加号；标准是一页10个paper，so start表示从第几页开始\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    results = []  # A list for storing results\n",
    "    total_articles = 0\n",
    "    already_exists_count = 0\n",
    "    title_none_count = 0\n",
    "    added_count = 0\n",
    "\n",
    "    response = requests.get(base_url, params=query_params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        divs = soup.find_all(\"div\", class_=\"gs_r gs_or gs_scl\")\n",
    "\n",
    "        for div in divs:\n",
    "            total_articles += 1\n",
    "            result = {}\n",
    "            h3 = div.find(\"h3\")\n",
    "            if h3:\n",
    "                a = h3.find(\"a\")  # 爬取文章名称和href(链接地址)\n",
    "                if a:\n",
    "                    title = a.get_text()\n",
    "                    href = a.get('href')\n",
    "                    if title in df_persist['Title'].values:\n",
    "                        already_exists_count += 1\n",
    "                        continue  # 如果标题已经存在，跳过这个结果\n",
    "                    result[\"Title\"] = title\n",
    "                    result[\"href\"] = href\n",
    "                else:\n",
    "                    title_none_count += 1\n",
    "                    result[\"Title\"] = None\n",
    "                    result[\"href\"] = None\n",
    "            else:\n",
    "                title_none_count += 1\n",
    "                result[\"Title\"] = None\n",
    "                result[\"href\"] = None\n",
    "\n",
    "            gs_a = div.find(\"div\", class_=\"gs_a\")  # 爬取作者 期刊 年份 期刊网址\n",
    "            if gs_a:\n",
    "                basic_info = gs_a.get_text()\n",
    "                result[\"basic_info\"] = basic_info\n",
    "\n",
    "            gs_rs = div.find(\"div\", class_=\"gs_rs\")  # 爬取一部分摘要\n",
    "            if gs_rs:\n",
    "                abstract = gs_rs.get_text()\n",
    "                result[\"concise_abstract\"] = abstract\n",
    "\n",
    "            gs_or_nvi = div.find(\"a\", class_=\"gs_or_nvi\")  # 爬取网页快照上的完整版摘要\n",
    "            if gs_or_nvi:\n",
    "                snapshot = gs_or_nvi.get('href')\n",
    "                if snapshot != \"javascript:void(0)\":  # 如果爬取下来的是一个网址，再到二级网址中爬取具体的摘要\n",
    "                    second_response = requests.get(snapshot)\n",
    "                    if second_response.status_code == 200:\n",
    "                        second_soup = BeautifulSoup(second_response.content, \"html.parser\")\n",
    "                        article_abstract_div = second_soup.find(\"div\", id=\"articleAbstract\")\n",
    "                        if article_abstract_div is not None:\n",
    "                            articleAbstract = article_abstract_div.get_text()\n",
    "                            result[\"full_abstract\"] = articleAbstract\n",
    "                    else:\n",
    "                        print(\"An error occurred while crawling the secondary URL!\", second_response.status_code)\n",
    "\n",
    "            results.append(result)  # Add the current result to the list\n",
    "            if result[\"Title\"] is not None and result[\"Title\"] not in df_persist['Title'].values:\n",
    "                df_persist.loc[len(df_persist)] = result  # Add new result to the persistent DataFrame\n",
    "                added_count += 1\n",
    "    else:\n",
    "        print(\"Error\", response.status_code)\n",
    "\n",
    "    df_persist = df_persist.dropna(subset=['Title']) # 删除Title列为NaN的行\n",
    "    df_persist.to_csv(persist_file, index=False)  # Save the updated DataFrame to CSV\n",
    "    print(f\"Total articles found: {total_articles}\")\n",
    "    print(f\"{already_exists_count} entries already existed in the persistent storage.\")\n",
    "    print(f\"{title_none_count} articles had no title.\")\n",
    "    print(f\"Actually added {added_count} new entries to the persistent storage.\")\n",
    "    return results  # Return result list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0b14f-8d82-4489-b785-5c06e51e823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例调用\n",
    "results = fetch_google_scholar_results(\"全球胜任力培养\", 2023, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa86f35-a12d-4020-8fca-fb668499bd61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1461a-5495-44f6-a9ac-c76133074100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207632e-cff4-4d6f-8f3a-9c3c5c85761f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
