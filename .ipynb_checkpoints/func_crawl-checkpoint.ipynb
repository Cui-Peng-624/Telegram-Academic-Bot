{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae9fae0c-bb07-4390-bb86-cd035b5bbeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"]=\"127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"]=\"127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c87088b8-fec5-4068-8d2d-15163a29e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 官方API\n",
    "# client = OpenAI()\n",
    "\n",
    "# 第三方低价API\n",
    "api_key = \"sk-FCDYB07IqQG164cC9604D8096d2e4e44B7CdF8684e19A51f\"\n",
    "api_base = \"https://api.zetatechs.com/v1\"\n",
    "client = OpenAI(api_key=api_key, base_url=api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a9a191-4163-4758-b287-e19a40a9536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telebot\n",
    "\n",
    "BOT_TOKEN = \"7256487851:AAGZKPYt-UsaPStdNU1MfUdC9XE5GikcYDY\"\n",
    "bot = telebot.TeleBot(BOT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd556c80-f649-4f36-b6cf-972b50f78bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@bot.message_handler(commands=['crawl'])\n",
    "def initiate_crawl(message):\n",
    "    text = \"您好！在爬取之前您需要回答以下问题：\\n1.您需要爬取的文章的内容与什么有关？\\n2.您需要爬取多少篇文章？\\n3.您是否需要限制爬取的文章为几几年之后发表的？如果需要限制，请显式指出年份信息。\\n4.您需要在中国知网还是谷歌学术查询论文？\\n注意：\\n1.请使用英文进行说明。\\n2.如果不限制年份则默认从2000年开始。如果不限制查询数量，则默认10。\\n3.建议查询不超过20篇文章，因为爬取非常耗费时间。\"\n",
    "    sent_msg = bot.send_message(message.chat.id, text, parse_mode=\"Markdown\")\n",
    "    bot.register_next_step_handler(sent_msg, process_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c17a02-53b1-4e76-94f3-6ec2184d0d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "response_format_extract_keywords_and_fields_and_as_ylo_and_number_and_source = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"response_format_extract_keywords_fields_as_ylo_number_source\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"keywords_for_searching\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"从用户请求中提取用于查询文章的关键字。例子: User request: 我需要爬取26篇有关全球胜任力的文章，并且最好都是2020年之后发表的。最好从谷歌学术上爬取。 - Extracted keywords: 全球胜任力\"\n",
    "                },\n",
    "                \"fields_for_searching\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"从用户请求中判断用户需要查询的是哪一个领域的文章。例子: User request: 我需要爬取26篇有关全球胜任力的文章，并且最好都是2020年之后发表的。最好从谷歌学术上爬取。 - Extracted fields: Eduction\",\n",
    "                    \"enum\": [\"Eduction\", \"Computer_Science\", \"Medicine\", \"Literature\", \"Other\"]\n",
    "                },\n",
    "                \"as_ylo_for_searching\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"从用户请求中提取的年份信息，提取结果必须为数字。例子: User request: 我需要爬取26篇有关全球胜任力的文章，并且最好都是2020年之后发表的。最好从谷歌学术上爬取。 - Extracted as_ylo: 2020\"\n",
    "                },\n",
    "                \"number_for_searching\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"从用户请求中提取用户需要爬取多少篇文章，提取结果必须为数字。例子: User request: 我需要爬取26篇有关全球胜任力的文章，并且最好都是2020年之后发表的。最好从谷歌学术上爬取。 - Extracted number: 26\"\n",
    "                },\n",
    "                \"source_for_searching\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"从用户请求中提取用户想要查询论文的来源。例子: User request: 我需要爬取26篇有关全球胜任力的文章，并且最好都是2020年之后发表的。最好从谷歌学术上爬取。 - Extracted source: Google_Scholar\",\n",
    "                    \"enum\": [\"CNKI\", \"Google_Scholar\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"keywords_for_searching\", \"fields_for_searching\", \"as_ylo_for_searching\", \"number_for_searching\", \"source_for_searching\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "class Keywords_and_Fields_and_As_Ylo_and_Number_and_Source(BaseModel):\n",
    "    keywords_for_searching: str\n",
    "    fields_for_searching: str\n",
    "    as_ylo_for_searching: str\n",
    "    number_for_searching: str\n",
    "    source_for_searching: str\n",
    "\n",
    "response_format_final_fields = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"response_format_final_fields\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"final_fields\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"参考我们训练的文本分类模型的预测和gpt-4o-mini-2024-07-18的预测，并根据原始的user_request，对用户需要查询的是哪一个领域的文章进行最终预测。\",\n",
    "                    \"enum\": [\"Eduction\", \"Computer_Science\", \"Medicine\", \"Literature\", \"Other\"]\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"final_fields\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "class Final_Fields(BaseModel):\n",
    "    final_fields: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5556aa99-bd7d-4ad4-b95c-ff9c33f98f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20703\\generative-ai-for-beginners\\MyTest\\save_load_checkpoint.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and optimizer states from checkpoints/TextCNN/TextCNN.pth and checkpoints/TextCNN/optimizer.pth, starting from epoch 79.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from requests.exceptions import SSLError\n",
    "\n",
    "from request_processor import request_processor\n",
    "\n",
    "def process_query(message): \n",
    "    user_request = message.text # 用户请求类似 \"I need to crawl 26 articles about global competency, preferably all published after 2020. Better to crawl it from Google Scholar.\"\n",
    "\n",
    "    messages_to_model=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert at structured data extraction. You will be given unstructured text from a user request and should convert it into the given structure.\"},\n",
    "        {\"role\": \"user\", \"content\": user_request}\n",
    "    ]\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-mini-2024-07-18\", messages=messages_to_model, response_format=response_format_extract_keywords_and_fields_and_as_ylo_and_number_and_source)\n",
    "    \n",
    "    try:\n",
    "        keywords_and_fields_and_as_ylo_and_number_and_source = Keywords_and_Fields_and_As_Ylo_and_Number_and_Source.parse_raw(response.choices[0].message.content)\n",
    "        # print(\"@@@@@@\", keywords_and_fields_and_as_ylo_and_number)\n",
    "        keywords_and_fields_and_as_ylo_and_number_and_source_dict = keywords_and_fields_and_as_ylo_and_number_and_source.dict()\n",
    "        keywords = keywords_and_fields_and_as_ylo_and_number_and_source_dict[\"keywords_for_searching\"] # str\n",
    "        fields = keywords_and_fields_and_as_ylo_and_number_and_source_dict[\"fields_for_searching\"] # str\n",
    "        as_ylo = keywords_and_fields_and_as_ylo_and_number_and_source_dict[\"as_ylo_for_searching\"] # str\n",
    "        number = keywords_and_fields_and_as_ylo_and_number_and_source_dict[\"number_for_searching\"] # str\n",
    "        source = keywords_and_fields_and_as_ylo_and_number_and_source_dict[\"source_for_searching\"] # str\n",
    "        # print(\"******\", keywords, fields, as_ylo, number)\n",
    "    except ValidationError as e:\n",
    "        print(\"### Keywords_and_Fields_and_As_Ylo_and_Number.parse_raw(response.choices[0].message.content) 出现错误 ###\", e.json())\n",
    "\n",
    "    ########################## gpt-4o-mini + 文本分类模型 -> gpt-4o 综合判断 ##########################\n",
    "    predicted_field, predicted_prob = request_processor(user_request) # 使用我们的文本分类模型判断用户请求属于哪一个fields\n",
    "    predicted_field, predicted_prob = str(predicted_field), str(predicted_prob)\n",
    "    \n",
    "    user_messages_for_final_fields = f\"用户请求为：{user_request}。我们训练的文本分类模型判断用户请求属于：{predicted_field}。gpt-4o-mini判断用户请求属于：{fields}\"\n",
    "    \n",
    "    # 将gpt-4o-mini-2024-07-18判断的fields和文本分类模型判断的fields发送给gpt-4o-2024-08-06(使用此版本仅为了为了适应json格式的输出)，让gpt-4o-2024-08-06最终判断一下fields\n",
    "    messages_for_final_fields = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in text classification and decision-making. You will be provided with a user request, along with predictions from two models regarding the likely field or category of the request. Your task is to analyze the provided information and make a final, authoritative decision on which category the user request belongs to. Consider the input from both models carefully before making your decision.\"},\n",
    "        {\"role\": \"user\", \"content\": user_messages_for_final_fields}\n",
    "    ]\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-2024-08-06\", messages=messages_for_final_fields, response_format=response_format_final_fields)\n",
    "\n",
    "    try:\n",
    "        final_fields = Final_Fields.parse_raw(response.choices[0].message.content)\n",
    "        final_fields_dict = final_fields.dict()\n",
    "        final_fields = final_fields_dict[\"final_fields\"]\n",
    "    except ValidationError as e:\n",
    "        print(\"### Final_Fields.parse_raw(response.choices[0].message.content) 出现错误 ###\", e.json())\n",
    "    ########################## gpt-4o-mini + 文本分类模型 -> gpt-4o 综合判断 ##########################\n",
    "    \n",
    "    # 如果是无所谓数量和年份，则返回为空，我们必须处理这种情况\n",
    "    if not as_ylo:  # 检查 as_ylo 是否为 None 或者空字符串\n",
    "        # print(\"as_ylo is None or empty, 默认使用2000\")\n",
    "        as_ylo = \"2000\"\n",
    "    if not number:  # 检查 number 是否为 None 或者空字符串\n",
    "        # print(\"number is None or empty, 默认使用10\")\n",
    "        number = \"10\"\n",
    "\n",
    "    text = f\"我们判断您需要从{source}查询{final_fields}领域的，与{keywords}有关的从{as_ylo}开始的{number}篇文章。请问是否正确，如果正确我们将开始查询，如果错误，请即使指出！\"\n",
    "    # 为防止网络问题导致的问题，使用try提高下容错\n",
    "    try:\n",
    "        sent_msg = bot.send_message(message.chat.id, text, parse_mode=None)\n",
    "    except SSLError as e:\n",
    "        print(f\"SSL error occurred when bot.send_message(): {e}. Retrying...\")\n",
    "        time.sleep(5)  # 等待几秒后重试\n",
    "        sent_msg = bot.send_message(message.chat.id, text, parse_mode=None)\n",
    "\n",
    "    bot.register_next_step_handler(sent_msg, confirm_and_show_extracted_information, keywords, final_fields, as_ylo, number, source, user_request, fields, predicted_field) # gpt4omini - fields; 文本分类模型 - predicted_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23955b55-c35f-4cea-82c1-72775a319750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据用户反馈判断提取出来的信息是否出错\n",
    "response_format_is_correct = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"response_format_is_correct\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"is_correct\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Indicates whether the user approves the extracted information.\",\n",
    "                    \"enum\": [True, False]\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"is_correct\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "class UserApproval(BaseModel):\n",
    "    is_correct: bool\n",
    "\n",
    "# 加入提取出来的信息出现了错误，则根据提取出来的信息和用户反馈对错误信息进行修正\n",
    "response_format_true_info = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"response_format_true_real_info\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"true_keywords\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The confirmed or corrected keywords based on the extracted information and the user's feedback.\"\n",
    "                },\n",
    "                \"true_fields\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The confirmed or corrected field based on the extracted information and the user's feedback.\",\n",
    "                    \"enum\": [\"Eduction\", \"Computer_Science\", \"Medicine\", \"Literature\", \"Other\"]\n",
    "                },\n",
    "                \"true_as_ylo\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The confirmed or corrected as_ylo (year lower bound) based on the extracted information and the user's feedback.\"\n",
    "                },\n",
    "                \"true_number\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The confirmed or corrected number of articles to crawl based on the extracted information and the user's feedback.\"\n",
    "                },\n",
    "                \"true_source\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The confirmed or corrected source (e.g., CNKI, Google Scholar) based on the extracted information and the user's feedback.\",\n",
    "                    \"enum\": [\"CNKI\", \"Google_Scholar\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"true_keywords\", \"true_fields\", \"true_as_ylo\", \"true_number\", \"true_source\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "class TrueInfo(BaseModel):\n",
    "    true_keywords: str\n",
    "    true_fields: str\n",
    "    true_as_ylo: str\n",
    "    true_number: str\n",
    "    true_source: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9605c78-60fa-4bd4-8e0d-7ed125b9aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AppendRowtoCSV import append_row_to_csv\n",
    "\n",
    "def confirm_and_show_extracted_information(message, keywords, final_fields, as_ylo, number, source, user_request, fields, predicted_field): # gpt4omini - fields; 文本分类模型 - predicted_field\n",
    "    message_text = message.text  # 示例：1.判断正确的。2.不对，是有关计算机领域！\n",
    "\n",
    "    messages_to_model=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in information validation. The user has been presented with extracted information (keywords, fields, etc.). Your task is to analyze the user's feedback and determine whether they confirm the correctness of the presented information or not.\"},\n",
    "        {\"role\": \"user\", \"content\": message_text}\n",
    "    ]\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-2024-08-06\", messages=messages_to_model, response_format=response_format_is_correct)\n",
    "\n",
    "    try:\n",
    "        is_correct = UserApproval.parse_raw(response.choices[0].message.content)\n",
    "        is_correct_dict = is_correct.dict()\n",
    "        is_correct = is_correct_dict[\"is_correct\"]\n",
    "    except ValidationError as e:\n",
    "        print(\"### UserApproval.parse_raw(response.choices[0].message.content) 出现错误 ###\", e.json())\n",
    "        return\n",
    "\n",
    "    # 如果正确 ###############################################################################################################################\n",
    "    if is_correct: \n",
    "        \n",
    "        if predicted_field != final_fields: # 如果用户认为final_fields正确，但我们的文本分类模型判断错了，则需要记录一下\n",
    "            csv_file = f'data/data_misclassified_user_requests.csv'\n",
    "            new_row = {'Text': user_request, 'Field_true': final_fields, \"Field_predictedbyModel\": predicted_field}\n",
    "            append_row_to_csv(csv_file, new_row)\n",
    "        if fields != final_fields: # 如果用户认为final_fields正确，但gpt-4o-mini判断错了，则也需要记录一下 - 暂时没写 - 暂时规划：判断错的信息作为prompt返回给gpt-4o-mini\n",
    "            pass\n",
    "            \n",
    "        bot.send_message(message.chat.id, \"Very Good! 开始查询，请稍后。\", parse_mode=None)\n",
    "        \n",
    "        #################################### 爬取 ####################################\n",
    "        df_crawled = crawl_details(source, keywords, final_fields, as_ylo, number)\n",
    "        #################################### 爬取 ####################################\n",
    "        \n",
    "        bot.send_message(message.chat.id, \"查询完成，请等待模型总结查询到的内容。\", parse_mode=None)\n",
    "        ###### 展示搜索到的所有结果 ######\n",
    "        crawled_text = convert_csv_to_text(df_crawled, source)\n",
    "        # print(\"!!!\", crawled_text)\n",
    "        # 将查询的到结果进行简化\n",
    "        if crawled_text == \"网站暂时无法爬取，请稍后！\": # 与 convert_csv_to_text() 中对应\n",
    "            bot.send_message(message.chat.id, \"网站暂时无法爬取，请稍后！\", parse_mode=None)\n",
    "        else:\n",
    "            summarized_text = summarize_and_format_articles(crawled_text, source)\n",
    "            # print(\"@@@\", summarized_text)\n",
    "            # 将summarized_text拆分成最大长度为4096的chunk进行发送\n",
    "            text_chunks = split_text_into_chunks(summarized_text)\n",
    "            for chunk in text_chunks:\n",
    "                bot.send_message(message.chat.id, chunk, parse_mode=\"Markdown\")\n",
    "            sent_msg = bot.send_message(message.chat.id, \"以上是本次全部的查询内容。本次查询的数据储存在临时数据库中，是否需要合并到永久数据库中？\\n合并可用于类似情况的推荐。\", parse_mode=\"Markdown\")\n",
    "            bot.register_next_step_handler(sent_msg, merge_data, final_fields)\n",
    "    \n",
    "    # 如果错误 ###############################################################################################################################\n",
    "    else: \n",
    "        text = f\"我们提取出来的信息为：1.用于查询的关键字：{keywords}；2.查询属于的领域：{final_fields}；3.文章的发表年份需要在{as_ylo}年以后；4.需要爬取{number}篇文章。5.需要从{source}查询文章。用户对我们提取的信息的反馈为：{message_text}\"\n",
    "        messages_to_model = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in information validation. Based on the user's feedback, correct only the parts of the extracted information that are incorrect and confirm the others.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "        response = client.chat.completions.create(model=\"gpt-4o-2024-08-06\", messages=messages_to_model, response_format=response_format_true_info)\n",
    "\n",
    "        try:\n",
    "            true_info = TrueInfo.parse_raw(response.choices[0].message.content)\n",
    "            true_info_dict = true_info.dict()\n",
    "            keywords = true_info_dict[\"true_keywords\"] # str\n",
    "            final_fields = true_info_dict[\"true_fields\"] # str\n",
    "            as_ylo = true_info_dict[\"true_as_ylo\"] # str\n",
    "            number = true_info_dict[\"true_number\"] # str\n",
    "            source = true_info_dict[\"true_source\"] # str\n",
    "            bot.send_message(message.chat.id, f\"非常抱歉造成错误，我们已经记录了错误信息！Corrected Information: keywords={keywords}, fields={final_fields}, as_ylo={as_ylo}, number={number}, source={source}。开始查询，请稍后！\", parse_mode=None)\n",
    "        except ValidationError as e:\n",
    "            print(\"### TrueInfo.parse_raw(response.choices[0].message.content) 出现错误 ###\", e.json())\n",
    "            return\n",
    "\n",
    "        if predicted_field != final_fields: # 如果用户认为final_fields正确，但我们的文本分类模型判断错了，则需要记录一下\n",
    "            csv_file = f'data/data_misclassified_user_requests.csv'\n",
    "            new_row = {'Text': user_request, 'Field_true': final_fields, \"Field_predictedbyModel\": predicted_field}\n",
    "            append_row_to_csv(csv_file, new_row)\n",
    "        if fields != final_fields: # 如果用户认为final_fields正确，但gpt-4o-mini判断错了，则也需要记录一下 - 暂时没写 - 暂时规划：判断错的信息作为prompt返回给gpt-4o-mini\n",
    "            pass\n",
    "\n",
    "        #################################### 爬取 ####################################\n",
    "        df_crawled = crawl_details(source, keywords, final_fields, as_ylo, number)\n",
    "        #################################### 爬取 ####################################\n",
    "        \n",
    "        bot.send_message(message.chat.id, \"查询完成，请等待模型总结查询到的内容。\", parse_mode=None)\n",
    "        ###### 展示搜索到的所有结果 ######\n",
    "        crawled_text = convert_csv_to_text(df_crawled, source)\n",
    "        # print(\"!!!\", crawled_text)\n",
    "        # 将查询的到结果进行简化\n",
    "        if crawled_text == \"网站暂时无法爬取，请稍后！\": # 与 convert_csv_to_text() 中对应\n",
    "            bot.send_message(message.chat.id, \"网站暂时无法爬取，请稍后！\", parse_mode=None)\n",
    "        else:\n",
    "            summarized_text = summarize_and_format_articles(crawled_text, source)\n",
    "            # print(\"@@@\", summarized_text)\n",
    "            # 将summarized_text拆分成最大长度为4096的chunk进行发送\n",
    "            text_chunks = split_text_into_chunks(summarized_text)\n",
    "            for chunk in text_chunks:\n",
    "                bot.send_message(message.chat.id, chunk, parse_mode=\"Markdown\")\n",
    "            sent_msg = bot.send_message(message.chat.id, \"以上是本次全部的查询内容。本次查询的数据储存在临时数据库中，是否需要合并到永久数据库中？\\n合并可用于类似情况的推荐。\", parse_mode=\"Markdown\")\n",
    "            bot.register_next_step_handler(sent_msg, merge_data, final_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8af3a20-2342-4963-aec9-a46273b5eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def q_processor(input_string):\n",
    "    # 删除所有标点符号\n",
    "    cleaned_string = re.sub(r'[^\\w\\s]', '', input_string)  # \\w匹配字母数字字符，\\s匹配空白字符\n",
    "    # 将左右的空格转换为'+'\n",
    "    cleaned_string = re.sub(r'\\s+', '+', cleaned_string.strip())  \n",
    "    return cleaned_string\n",
    "\n",
    "from crawler.GoogleScholar import fetch_and_save_results_temp\n",
    "from crawler.CNKI import get_page_text,parse_page_text\n",
    "    \n",
    "def crawl_details(source, keywords, final_fields, as_ylo, number):\n",
    "    temp_save_path = f\"data/data_crawled/database_temporary/{final_fields}.csv\"  # 保存路径\n",
    "    q = q_processor(keywords)\n",
    "    if source == \"Google_Scholar\":\n",
    "        start = int(number) // 10 # 判断 number 是否可以被 10 整除\n",
    "        if int(number) % 10 != 0:\n",
    "            start += 1\n",
    "        # print(\"###\", q, as_ylo, start)\n",
    "        df_crawled = fetch_and_save_results_temp(temp_save_path, q, int(as_ylo), start)\n",
    "        # print(\"###\", df_crawled)\n",
    "        return df_crawled\n",
    "    elif source == \"CNKI\":\n",
    "        num_pages = int(number) // 20 + 1 # number除以20的整数部分 + 1\n",
    "        page_text = get_page_text(\"http://search.cnki.com.cn/Search/ListResult\", q, num_pages)\n",
    "        df_crawled = parse_page_text(page_text, int(number), temp_save_path)\n",
    "        # print(\"###\", df_crawled)\n",
    "        return df_crawled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cd17ddc-58e7-4282-b273-51f7c767f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def convert_csv_to_text(df, source):\n",
    "#     if df is None or df.empty:\n",
    "#         return \"网站暂时无法爬取，请稍后！\"\n",
    "\n",
    "#     total_rows = len(df)\n",
    "#     articles = []\n",
    "\n",
    "#     for i in range(total_rows):\n",
    "#         if source == \"Google_Scholar\":\n",
    "#             title = df.loc[i, 'Title']\n",
    "#             href = df.loc[i, 'href']\n",
    "#             basic_info = df.loc[i, 'basic_info']\n",
    "#             concise_abstract = df.loc[i, 'concise_abstract']\n",
    "#             full_abstract = df.loc[i, 'full_abstract']\n",
    "\n",
    "#             article = (\n",
    "#                 f\"搜索到的第{i+1}篇文章：\\n\"\n",
    "#                 f\"Title: {title}\\n\"\n",
    "#                 f\"Href: {href}\\n\"\n",
    "#                 f\"Basic Information: {basic_info}\\n\"\n",
    "#                 f\"Consise Abstract: {concise_abstract}\\n\"\n",
    "#             )\n",
    "\n",
    "#             if full_abstract != 'N/A':\n",
    "#                 article += f\"Full Abstract: {full_abstract}\\n\"\n",
    "\n",
    "#             print(\"::::::\", article)  # 保证每篇文章都输出\n",
    "            \n",
    "#         elif source == \"CNKI\":\n",
    "#             title = df.loc[i, 'Title']\n",
    "#             author = df.loc[i, 'Author']\n",
    "#             source = df.loc[i, 'Source']\n",
    "#             paper_type = df.loc[i, 'Type']\n",
    "#             date = df.loc[i, 'Date']\n",
    "#             abstract = df.loc[i, 'Abstract']\n",
    "#             keywords = df.loc[i, 'Keywords']\n",
    "#             download = df.loc[i, 'Download']\n",
    "#             citations = df.loc[i, 'Citations']\n",
    "#             link = df.loc[i, 'Link']\n",
    "\n",
    "#             # 初始化文章摘要\n",
    "#             article = f\"搜索到的第{i+1}篇文章：\\n\"\n",
    "\n",
    "#             # 按需添加非空字段\n",
    "#             if title != 'N/A':\n",
    "#                 article += f\"Title: {title}\\n\"\n",
    "#             if author != 'N/A':\n",
    "#                 article += f\"Author: {author}\\n\"\n",
    "#             if source != 'N/A':\n",
    "#                 article += f\"Source: {source}\\n\"\n",
    "#             if paper_type != 'N/A':\n",
    "#                 article += f\"Type: {paper_type}\\n\"\n",
    "#             if date != 'N/A':\n",
    "#                 article += f\"Date: {date}\\n\"\n",
    "#             if abstract != 'N/A':\n",
    "#                 article += f\"Abstract: {abstract}\\n\"\n",
    "#             if keywords != 'N/A':\n",
    "#                 article += f\"Keywords: {keywords}\\n\"\n",
    "#             if download != 'N/A':\n",
    "#                 article += f\"Download: {download}\\n\"\n",
    "#             if citations != 'N/A':\n",
    "#                 article += f\"Citations: {citations}\\n\"\n",
    "#             if link != 'N/A':\n",
    "#                 article += f\"Link: {link}\\n\"\n",
    "\n",
    "#             print(\"::::::\", article)  # 保证每篇文章都输出\n",
    "\n",
    "#         articles.append(article)\n",
    "\n",
    "#     return \"\\n\".join(articles)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def convert_csv_to_text(df, source):\n",
    "    if df is None or df.empty:\n",
    "        return \"网站暂时无法爬取，请稍后！\"\n",
    "\n",
    "    total_rows = len(df)\n",
    "    articles = []\n",
    "\n",
    "    for i in range(total_rows):\n",
    "        article = f\"搜索到的第{i+1}篇文章：\\n\"\n",
    "\n",
    "        if source == \"Google_Scholar\":\n",
    "            title = df.get('Title', pd.Series([None]*total_rows)).iloc[i]\n",
    "            href = df.get('href', pd.Series([None]*total_rows)).iloc[i]\n",
    "            basic_info = df.get('basic_info', pd.Series([None]*total_rows)).iloc[i]\n",
    "            concise_abstract = df.get('Abstract', pd.Series([None]*total_rows)).iloc[i]\n",
    "            full_abstract = df.get('full_abstract', pd.Series([None]*total_rows)).iloc[i]\n",
    "\n",
    "            # 按需添加非空字段\n",
    "            if pd.notna(title):\n",
    "                article += f\"Title: {title}\\n\"\n",
    "            if pd.notna(href):\n",
    "                article += f\"Href: {href}\\n\"\n",
    "            if pd.notna(basic_info):\n",
    "                article += f\"Basic Information: {basic_info}\\n\"\n",
    "            if pd.notna(concise_abstract):\n",
    "                article += f\"Consise Abstract: {concise_abstract}\\n\"\n",
    "            if pd.notna(full_abstract) and full_abstract != 'N/A':\n",
    "                article += f\"Full Abstract: {full_abstract}\\n\"\n",
    "\n",
    "        elif source == \"CNKI\":\n",
    "            title = df.get('Title', pd.Series([None]*total_rows)).iloc[i]\n",
    "            author = df.get('Author', pd.Series([None]*total_rows)).iloc[i]\n",
    "            paper_source = df.get('Source', pd.Series([None]*total_rows)).iloc[i]\n",
    "            paper_type = df.get('Type', pd.Series([None]*total_rows)).iloc[i]\n",
    "            date = df.get('Date', pd.Series([None]*total_rows)).iloc[i]\n",
    "            abstract = df.get('Abstract', pd.Series([None]*total_rows)).iloc[i]\n",
    "            keywords = df.get('Keywords', pd.Series([None]*total_rows)).iloc[i]\n",
    "            download = df.get('Download', pd.Series([None]*total_rows)).iloc[i]\n",
    "            citations = df.get('Citations', pd.Series([None]*total_rows)).iloc[i]\n",
    "            link = df.get('Link', pd.Series([None]*total_rows)).iloc[i]\n",
    "\n",
    "            # 按需添加非空字段\n",
    "            if pd.notna(title):\n",
    "                article += f\"Title: {title}\\n\"\n",
    "            if pd.notna(author):\n",
    "                article += f\"Author: {author}\\n\"\n",
    "            if pd.notna(paper_source):\n",
    "                article += f\"Source: {paper_source}\\n\"\n",
    "            if pd.notna(paper_type):\n",
    "                article += f\"Type: {paper_type}\\n\"\n",
    "            if pd.notna(date):\n",
    "                article += f\"Date: {date}\\n\"\n",
    "            if pd.notna(abstract):\n",
    "                article += f\"Abstract: {abstract}\\n\"\n",
    "            if pd.notna(keywords):\n",
    "                article += f\"Keywords: {keywords}\\n\"\n",
    "            if pd.notna(download):\n",
    "                article += f\"Download: {download}\\n\"\n",
    "            if pd.notna(citations):\n",
    "                article += f\"Citations: {citations}\\n\"\n",
    "            if pd.notna(link):\n",
    "                article += f\"Link: {link}\\n\"\n",
    "\n",
    "        # 确保每篇文章都输出到控制台\n",
    "        # print(\"::::::\", article)\n",
    "\n",
    "        # 将每篇文章添加到列表\n",
    "        articles.append(article)\n",
    "\n",
    "    # 将所有文章内容合并并返回\n",
    "    return \"\\n\".join(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8faef1f1-6b2d-4a04-9ae1-e406d6673c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test convert_csv_to_text()\n",
    "# df = pd.read_csv(\"data/data_crawled/database_temporary/Eduction.csv\")\n",
    "# articles = convert_csv_to_text(df, \"Google_Scholar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa1eef40-68c3-4f7f-a574-b1579b323d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_format_articles(text, source):\n",
    "    if source == \"Google_Scholar\":\n",
    "        system_message_content = (\n",
    "            \"You are an expert in summarizing academic articles. The user will provide you with details of several articles including title, href, \"\n",
    "            \"basic information(including authors, year, journal), and abstract(consise abstract is always available, but the full abstract may not be available). \"\n",
    "            \"Your task is to generate a concise summary of each article, focusing on key details and simplifying the abstract where possible. \"\n",
    "            \"Format the output in the following manner:\\n\\n\"\n",
    "            \"1. **Title of the Article**\\n\"\n",
    "            \"   - **Author:** Author's Name\\n\"\n",
    "            \"   - **Year:** Year of Publication\\n\"\n",
    "            \"   - **Journal:** Journal Name\\n\"\n",
    "            \"   - **Href:** href\\n\"\n",
    "            \"   - **Summary:** Simplified abstract or summary of the article.\\n\\n\"\n",
    "            \"Ensure each article follows this structure, and keep the summaries brief and clear.\"\n",
    "        )\n",
    "    elif source == \"CNKI\":\n",
    "        system_message_content = (\n",
    "            \"You are an expert in summarizing academic articles from CNKI. The user will provide you with details of several articles including title, author, source (journal), \"\n",
    "            \"type of the article, date of publication, abstract, keywords, download and citation counts, and link. Your task is to generate a concise summary of each article, \"\n",
    "            \"focusing on key details and simplifying the abstract where possible. Format the output in the following manner:\\n\\n\"\n",
    "            \"1. **Title of the Article**\\n\"\n",
    "            \"   - **Author:** Author's Name\\n\"\n",
    "            \"   - **Source (Journal):** Journal Name\\n\"\n",
    "            \"   - **Type:** Type of the article\\n\"\n",
    "            \"   - **Date:** Date of Publication\\n\"\n",
    "            \"   - **Keywords:** Keywords associated with the article\\n\"\n",
    "            \"   - **Download Count:** Number of downloads\\n\"\n",
    "            \"   - **Citation Count:** Number of citations\\n\"\n",
    "            \"   - **Link:** Article link\\n\"\n",
    "            \"   - **Summary:** Simplified abstract or summary of the article.\\n\\n\"\n",
    "            \"Ensure each article follows this structure, and keep the summaries brief and clear.\"\n",
    "        )\n",
    "    else:\n",
    "        return \"Invalid source specified.\"\n",
    "\n",
    "    # Construct messages based on the source\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message_content},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    \n",
    "    # Call the API to summarize and format the articles\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    summary = response.choices[0].message.content\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d35030-597f-4240-8b09-62e1259be8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解决 Bad Request: message is too long 的问题 - Telegram 发送消息时的字符限制是 4096 个字符\n",
    "def split_text_into_chunks(text, chunk_size=4096):\n",
    "    chunks = []\n",
    "    \n",
    "    while len(text) > chunk_size:\n",
    "        # 尝试在chunk_size之前找到最后一个空行位置\n",
    "        split_index = text[:chunk_size].rfind('\\n\\n')\n",
    "        \n",
    "        # 如果没有找到空行，则回退到最后一个单独换行符处\n",
    "        if split_index == -1:\n",
    "            split_index = text[:chunk_size].rfind('\\n')\n",
    "        \n",
    "        # 如果找不到单独的换行符，默认拆分点为chunk_size\n",
    "        if split_index == -1:\n",
    "            split_index = chunk_size\n",
    "        \n",
    "        chunks.append(text[:split_index].strip())  # 去除末尾多余的空格\n",
    "        text = text[split_index:].strip()  # 去除开头多余的空格\n",
    "    \n",
    "    # 添加最后的剩余部分\n",
    "    chunks.append(text.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be78a4ef-a4d3-446e-aaac-7eb154d7f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "# 定义请求格式\n",
    "response_format_merge_request_schema = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"merge_request_schema\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"merge_to_permanent\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Indicates whether the user wants to merge the temporary database into the permanent database.\",\n",
    "                    \"enum\": [True, False]\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"merge_to_permanent\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# 定义 Pydantic 模型\n",
    "class MergeRequest(BaseModel):\n",
    "    merge_to_permanent: bool\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 合并数据的函数\n",
    "def merge_data(message, final_fields):\n",
    "    message_text = message.text  # 类似：1.是的需要合并；2.不需要合并\n",
    "\n",
    "    messages_to_model = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in database management. The user has a temporary database, and your task is to analyze the user's request and determine whether they want to merge this temporary database into the permanent database.\"},\n",
    "        {\"role\": \"user\", \"content\": message_text}\n",
    "    ]\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-mini-2024-07-18\", messages=messages_to_model, response_format=response_format_merge_request_schema)\n",
    "\n",
    "    try:\n",
    "        merge_request = MergeRequest.parse_raw(response.choices[0].message.content)\n",
    "        merge_request_dict = merge_request.dict()\n",
    "        merge_to_permanent = merge_request_dict[\"merge_to_permanent\"]\n",
    "    except ValidationError as e:\n",
    "        print(\"### MergeRequest.parse_raw(response.choices[0].message.content) 出现错误 ###\", e.json())\n",
    "        return\n",
    "\n",
    "    if merge_to_permanent:  # 需要合并\n",
    "        temp_file_path = f\"data/data_crawled/database_temporary/{final_fields}.csv\"\n",
    "        perm_file_path = f\"data/data_crawled/database_persistent/{final_fields}.csv\"\n",
    "\n",
    "        # 检查临时数据库中的文件是否存在\n",
    "        if not os.path.exists(temp_file_path):\n",
    "            print(f\"### 临时数据库中的文件 {temp_file_path} 不存在 ###\")\n",
    "            return\n",
    "\n",
    "        # 读取临时数据库中的 CSV 文件\n",
    "        temp_df = pd.read_csv(temp_file_path)\n",
    "\n",
    "        # 如果永久数据库中没有对应的 CSV 文件，则直接保存\n",
    "        if not os.path.exists(perm_file_path):\n",
    "            temp_df.to_csv(perm_file_path, index=False)\n",
    "            print(f\"### {final_fields}.csv 已合并到永久数据库 ###\")\n",
    "        else:\n",
    "            # 读取永久数据库中的 CSV 文件\n",
    "            perm_df = pd.read_csv(perm_file_path)\n",
    "\n",
    "            # 获取所有的列名（包括 Google Scholar 和 CNKI 的列）\n",
    "            all_columns = list(set(temp_df.columns).union(set(perm_df.columns)))\n",
    "\n",
    "            # 将缺失的列填充为 None，保证两个数据框的列一致\n",
    "            for column in all_columns:\n",
    "                if column not in temp_df.columns:\n",
    "                    temp_df[column] = None\n",
    "                if column not in perm_df.columns:\n",
    "                    perm_df[column] = None\n",
    "\n",
    "            # 进行合并并去重\n",
    "            merged_df = pd.concat([perm_df, temp_df], ignore_index=True)\n",
    "            merged_df.drop_duplicates(inplace=True)\n",
    "\n",
    "            # 保存合并后的数据到永久数据库\n",
    "            merged_df.to_csv(perm_file_path, index=False)\n",
    "            print(f\"### {final_fields}.csv 已与永久数据库中的现有文件合并 ###\")\n",
    "\n",
    "        print(\"### 数据集已成功合并到永久数据库 ###\")\n",
    "        bot.send_message(message.chat.id, \"合并完成！\", parse_mode=None)\n",
    "    else:\n",
    "        print(\"### 用户选择不合并临时数据库 ###\")\n",
    "        bot.send_message(message.chat.id, \"好的，本次查询内容将不会被合并。\", parse_mode=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "183ccd37-d654-4d7c-86f6-1a6cafaef5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ec6e06-7f31-4b04-b971-c868492f62d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功保存10条结果到data/data_crawled/database_temporary/Eduction.csv\n",
      "### Eduction.csv 已合并到永久数据库 ###\n",
      "### 数据集已成功合并到永久数据库 ###\n",
      "成功保存10条结果到data/data_crawled/database_temporary/Eduction.csv\n",
      "### Eduction.csv 已与永久数据库中的现有文件合并 ###\n",
      "### 数据集已成功合并到永久数据库 ###\n"
     ]
    }
   ],
   "source": [
    "# bot.infinity_polling()\n",
    "bot.polling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639c94c-6133-4455-bb88-8b39643e1264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcaade-696e-4600-a965-4abb10b7b191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa32fb-2a39-4f29-8ff1-cc93a46c8a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f210e480-9224-45a2-9708-487812286e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch_and_save_results_temp(\"temp_scholar_results.csv\", \"machine+learning\", 2020, 10)\n",
    "# user_confirmation = input(\"是否将数据保存到永久文件夹？(y/n): \").lower() == 'y'\n",
    "# confirm_and_save_results(\"temp_scholar_results.csv\", \"permanent_scholar_results.csv\", user_confirmation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c63ec-3c65-4857-9612-31371da43c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ca31f4a-e1c4-445b-ae3d-18bcb071d58c",
   "metadata": {},
   "source": [
    "这个报错信息表明在你尝试通过 Telegram Bot API 发送消息时，出现了 SSL/TLS 连接相关的问题。具体来说，问题可能出现在以下几个方面：\r\n",
    "\r\n",
    "### 1. **SSL/TLS 连接问题**\r\n",
    "   - **SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF)')**: 这个错误表明 SSL/TLS 连接在传输过程中被意外关闭了，通常是因为网络连接不稳定或者服务器关闭了连接。这可能是由于网络波动、API 请求超时，或者服务器（Telegram API）端的临时问题。\r\n",
    "\r\n",
    "### 2. **重试失败**\r\n",
    "   - **MaxRetryError**: 这个错误表明在尝试重新建立连接时达到最大重试次数，但仍未成功。通常意味着网络环境在一段时间内不稳定，或者目标服务器（Telegram API）无法响应。\r\n",
    "\r\n",
    "### 3. **HTTPSConnectionPool 相关问题**\r\n",
    "   - 这个错误发生在 `requests` 库尝试通过 HTTPS 连接 Telegram API 时，无法建立或维持一个稳定的 SSL/TLS 连接。\r\n",
    "\r\n",
    "### 解决方案和建议：\r\n",
    "\r\n",
    "1. **检查网络连接**：确保你的服务器或运行该脚本的机器网络连接正常，特别是能够稳定访问 Telegram API 服务器。如果是短暂的网络波动，稍等片刻再重试可能就能解决问题。\r\n",
    "\r\n",
    "2. **增加超时时间**：在 `telebot` 的 `polling` 或 `send_message` 函数中，可以考虑增加超时时间设置，以应对网络延迟问题。例如：\r\n",
    "\r\n",
    "   ```python\r\n",
    "   bot.polling(non_stop=True, timeout=10, long_polling_timeout=5)\r\n",
    "   ```\r\n",
    "\r\n",
    "3. **捕获并处理 SSL 错误**：你可以捕获这个 SSL 错误，并在出现错误时重试几次发送消息的操作：\r\n",
    "\r\n",
    "   ```python\r\n",
    "   import time\r\n",
    "   from requests.exceptions import SSLError\r\n",
    "\r\n",
    "   try:\r\n",
    "       sent_msg = bot.send_message(message.chat.id, text, parse_mode=\"Markdown\")\r\n",
    "   except SSLError as e:\r\n",
    "       print(f\"SSL error occurred: {e}. Retrying...\")\r\n",
    "       time.sleep(5)  # 等待几秒后重试\r\n",
    "       sent_msg = bot.send_message(message.chat.id, text, parse_mode=\"Markdown\")\r\n",
    "   ```\r\n",
    "\r\n",
    "4. **使用稳定的网络环境**：如果你是在本地运行代码，尝试使用更稳定的网络连接，或者在服务器端执行脚本。\r\n",
    "\r\n",
    "5. **检查 Telegram API 状态**：有时候问题可能是 Telegram API 的服务器端问题。你可以通过检查 Telegram 官方渠道或第三方 API 状态监控服务，确认是否有大规模的服务中断。\r\n",
    "\r\n",
    "通过以上方法，你可以有效地减少或解决这种 SSL/TLS 连接错误，并确保你的 Telegram 机器人能够稳定地发送消息。kdown\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2370ff1-7315-41a6-ab2a-18858257f7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e6dc1-7dc5-4532-aafd-8c36c1774655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e099c6-ae5c-474e-8ce8-f1175b9364b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e2329-f9ca-466c-b1cb-cbfff072c065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b46537-30cd-41c3-9ff9-f049fde65362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec802c2-c34c-4782-82e7-9fb75e33dd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca3c7b-8fea-49d1-a456-652404bffada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
